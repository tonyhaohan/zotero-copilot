<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs</title>
<!--Generated on Mon Nov  3 03:55:57 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://arxiv.org/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons_new.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
</head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S1" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Preliminaries</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2.SS1" title="In II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-A </span><span class="ltx_text ltx_font_italic">Rate-distortion Function</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2.SS2" title="In II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-B </span><span class="ltx_text ltx_font_italic">Directed Information</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2.SS3" title="In II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-C </span><span class="ltx_text ltx_font_italic">Granger Causality</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">LLM as a Next-token Predictor</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.SS1" title="In III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-A </span><span class="ltx_text ltx_font_italic">Probabilistic Model of LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.SS2" title="In III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-B </span><span class="ltx_text ltx_font_italic">Directed Rate-distortion Function in Pre-training Phase</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.SS3" title="In III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-C </span><span class="ltx_text ltx_font_italic">Directed Rate-reward Function in Post-training Phase</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.SS4" title="In III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-D </span><span class="ltx_text ltx_font_italic">Semantic Information Flow in Inference Phase</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Vector Representation of Token-level Semantics</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.SS1" title="In IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-A </span><span class="ltx_text ltx_font_italic">Token-level Semantic Space</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.SS2" title="In IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-B </span><span class="ltx_text ltx_font_italic">Token-level Semantic Vector Space</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.SS3" title="In IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-C </span><span class="ltx_text ltx_font_italic">Semantic Compression/De-dimensionality</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.SS4" title="In IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-D </span><span class="ltx_text ltx_font_italic">Semantic Embedding/Vectorization for Next-token Prediction</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Autoregression LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS1" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-A </span><span class="ltx_text ltx_font_italic">TV-VAR based AR-LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS2" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-B </span><span class="ltx_text ltx_font_italic">Transformer Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS3" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-C </span><span class="ltx_text ltx_font_italic">ELBO of the Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS4" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-D </span><span class="ltx_text ltx_font_italic">Generalization Error Bound of the Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS5" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E </span><span class="ltx_text ltx_font_italic">Memory Capacity of the Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS6" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F </span><span class="ltx_text ltx_font_italic">Semantic Information Theoretic Measure for the Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS7" title="In V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-G </span><span class="ltx_text ltx_font_italic">Other Architectures</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS7.SSS1" title="In V-G Other Architectures ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-G1 </span>Mamba/Mamba2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.SS7.SSS2" title="In V-G Other Architectures ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-G2 </span>LLaDA</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S6" title="In Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Bo¬†Bai
</span><span class="ltx_author_notes">Bo Bai, Lab Director and Chief Scientist of Information Theory, is with Theory Lab - Leibniz, Central Research Institute, 2012 Labs, Huawei Technology Co., Ltd., Hong Kong. Email: baibo8@hauwei.com</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Large language models (LLMs) have demonstrated remarkable capabilities in numerous real-world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate-distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">At the end of 2022, ChatGPT emerged and its capabilities stunned the entire world! A few month later, we fortunately invited Prof. Arikan, the inventor of Polar codes, for a panel discussion.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The event was broadcast live through the Chaspark website and became the best live event of that year.</span></span></span> My colleague, Dr. Wu, hosted the event, his first question was brilliant: ‚ÄúProf. Arikan, what do you consider the greatest invention of the information age?‚Äù After a moment of thought, the professor gave a decisive answer: ‚ÄúThe <span class="ltx_text ltx_font_bold">BIT</span>! I believe that the bit is the greatest invention of the information age.‚Äù This answer deeply shook me and has since inspired me to think about a question: What is the most important concept with the same fundamental importance as the bit in AI age, especially after ChatGPT emerged? After deeply involved into the research of LLMs, I finally realized that: the concept I am seeking is none other than the <span class="ltx_text ltx_font_bold">TOKEN</span>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Inspired by Shannon‚Äôs seminal 1948 paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib1" title="">1</a>]</cite>, I tried to approach the explanation theory of LLMs from inference perspective. Shannon started with the goal of achieving reliable information transmission in a communication system. From that starting point, he laid out a complete set of mathematical concepts and theorems, which is known as information theory. In 1949, Weaver and Shannon co-authored a paper in which they clearly identified three levels of communication problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib2" title="">2</a>]</cite>. They are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Level-A: Technical problem.</span> How accurately can the symbols of communication be transmitted?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Level-B: Semantic problem.</span> How precisely do the transmitted symbols convey the desired meaning?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Level-C: Effectiveness problem.</span> How effectively does the received meaning affect conduct in the desired way?</p>
</div>
</li>
</ul>
<p class="ltx_p">Shannon humbly suggested that his theory only solved the problem of reliable communication, i.e., Level-A technical problem. This is because, in Shannon‚Äôs theory, information is equivalent to uncertainty. He was not concerned with the meaning or significance of the transmitted message, but only with whether its binary representation was received without error. However, it is shown in our work that by extending Shannon‚Äôs theory to center on tokens, the underlying principles of LLMs can be explained from information-theoretic perspective, which will be referred to as semantic information theory.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Early research on semantics can be traced back to the work of Carnap, who had a series of brilliant discussions on this issue from the perspectives of empiricism, ontology, linguistics, and logic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib5" title="">5</a>]</cite>. In the classic book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib6" title="">6</a>]</cite>, Carnap provides a comprehensive and systematic exposition of semantics and modal logic. The modern developments of these approaches are well summarized in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib8" title="">8</a>]</cite>. Deeply influenced by Carnap, Solomonoff proposed the concept of algorithmic probability and integrated it into Bayesian inference framework, thereby providing a formal theory of inductive inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib11" title="">11</a>]</cite>. In Solomonoff‚Äôs theory, the prior probability of a sequence is determined by its complexity. Therefore, the shortest program that can generate the sequence has the highest prior probability, which is referred to as the universal prior. The length of this shortest program defined on a Turing machine is known as the Kolmogorov complexity of the sequence. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib13" title="">13</a>]</cite>, Kolmogorov complexity is introduced as a new logical basis for Shannon‚Äôs information theory based on computing complexity on a Turing machine. It can be seen that this is exactly about viewing a sequence from a generative perspective based on Turing machine. Based on the Solomonoff prior and Kolmogorov complexity, a universal reinforcement learning is proposed for sequence decision and AI agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib14" title="">14</a>]</cite>.
However, calculating the Kolmogorov complexity of a sequence is a Turing-undecidable problem, which in turn makes the theories of Kolmogorov and Solomonoff difficult to apply in practice.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">When we apply Kolmogorov complexity to the sample sequences of a random variable, the expected value is exactly the Shannon entropy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib15" title="">15</a>]</cite>. Therefore, it is believed that Shannon‚Äôs information theory is a probabilistic special case of Kolmogorov complexity theory. However, the probabilistic approach of information theory is more valuable for modern neural networks and LLMs, the core reason may lie in the computability of information-theoretic measures such as entropy, mutual information, and Kullback-Leibler (KL) divergence (or cross-entropy), and also the fact that they are easy to approximate from data in practice using other more easily computable quantities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib17" title="">17</a>]</cite>. This concept is precisely took away from Sutton‚Äôs famous short essay <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib18" title="">18</a>]</cite>, specifically the first sentence: ‚ÄúThe biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.‚Äù</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">A key question of extending Shannon‚Äôs theory to center on tokens is how to represent semantics of a token in a computable form. Unfortunately, source coding in Shannon‚Äôs theory only concerns how to represent the original message with the minimum number of binary symbols, but not with the semantics of the source. The idea of representing and retrieving information with vectors can be traced back to the work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib19" title="">19</a>]</cite>. The vector representation became the semantic basis of information-retrieval system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib20" title="">20</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib21" title="">21</a>]</cite>, Bengio et. al was the first to propose simultaneously learning a low-dimensional, distributed representation for words, i.e., a word vector, as part of training a language model. This marked the first time the concept of word vectors was combined with neural networks. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib22" title="">22</a>]</cite>, Mikolov et. al introduced two model architectures: CBOW and Skip-gram, which demonstrated that high-quality word vectors can be trained with great efficiency on massive text corpora using a simple neural network. In their following work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib23" title="">23</a>]</cite>, they showed that the learned word vectors exhibit linear substructures that capture meaningful semantic relationships between words. This finding was groundbreaking and sparked a wave of research on word embeddings, leading to the development of various models such as GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib24" title="">24</a>]</cite>, FastText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib25" title="">25</a>]</cite>, and ELMo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib26" title="">26</a>]</cite>. The vector representation of semantics has become the foundation of modern NLP and LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p">The vector representation, however, is only token-level semantics. How to extend the semantic representation and generation to a sentence, a paragraph, or even an article in a computing efficient way has long been a challenging problem. The advent of the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib28" title="">28</a>]</cite>, an architecture founded on the attention mechanism, represented a critical breakthrough, delivering extraordinary potential on NLP tasks. Subsequently, OpenAI introduced a series of GPT models built upon the Transformer architecture, which have exhibited remarkable capabilities in diverse applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib32" title="">32</a>]</cite>. Based on the classic Transformer architecture, DeepSeek has proposed a suite of enhancements aimed at substantially enhancing training efficiency. Consequently, the published LLMs exhibit remarkable inferential power <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib34" title="">34</a>]</cite>. However, there still lacks a deep theoretical understanding of the principles behind the Transformer architecture. Therefore, improving the architecture and further enhancing LLM capabilities relies heavily on large-scale experiments on GPUs, which in turn requires an immense investment of resources.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p">Numerous studies have found that information-theoretic methods have been applied to many aspects of machine learning and have played a significant role <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib35" title="">35</a>]</cite>. The information bottleneck method, employed to analyze the mechanics of deep learning, has gained significant attention within academia and industry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib36" title="">36</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib37" title="">37</a>]</cite>, the rate-distortion function and information bottleneck method are applied to explain the semantic embedding for LLMs. The language model based textual transform coding is proposed for sharply improving the compression performance of multimedia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib38" title="">38</a>]</cite>. To capture both the fidelity and the reality at the same time, the rate-distortion-perception function is surveyed for generative models in our work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib39" title="">39</a>]</cite>. The Transformer is modeled as an interacting particle system, with a particular emphasis on long-time clustering behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib40" title="">40</a>]</cite>. The centrality of data to LLM training underscores the significance of information-theoretic methods in data science, which is comprehensively reviewed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib41" title="">41</a>]</cite>. However, the autoregression LLM (AR-LLM), such as Transformer architecture, have not to be systematically studied from an information-theoretic perspective.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p">This paper leverages semantic information theory to construct a theoretical framework for understanding LLMs. We first propose a probabilistic model for LLM as a next-token predictor, which reveals it as a discrete-time channel with feedback and state. A significant modification to Shannon‚Äôs theory is to treat the channel as a generative model instead of a media for information transmission. The objective shifts from exactly recovering the original information to ensuring the generated sequence meets specific requirements. This perspective leads us to propose the directed rate-distortion function as a universal measure for LLMs in the pre-training phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib43" title="">43</a>]</cite>. The directed rate-reward function is also introduced for the reinforcement learning based post-training phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib44" title="">44</a>]</cite>, which shows that the LLM is approximating Granger causality at a human level for next-token prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib45" title="">45</a>]</cite>. The semantic information flow is defined and analyzed from the perspective of sub-martingale for the inference phase. Focusing on the foundations of LLMs, we then delve into the token-level semantic space and its vectorization. The semantic vector compression and the Gromov-Wasserstein distance based semantic distortion metric are discussed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib46" title="">46</a>]</cite>. Based on this groundwork, an information-theoretically optimal semantic vectorization method is introduced for next-token prediction. Its connection to contrastive predictive coding (CPC) is also examined <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib49" title="">49</a>]</cite>. Thereafter, premised on the theory of time-varying vector autoregression (TV-VAR) processes, we formally establish a general mathematical definition for AR-LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib50" title="">50</a>]</cite>. It is demonstrated that the Transformer architecture constitutes a specialized case of this general AR-LLM formulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib28" title="">28</a>]</cite>. Based on the variational inference principle, the evidence lower bound (ELBO) of Transformer is derived for both training phase and inference phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib51" title="">51</a>]</cite>. The generalization error bound for Transformer is analyzed by using Rademacher complexity and Talagrand inequality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib52" title="">52</a>]</cite>. The memory capacity, referred to as Gardner capacity for Hopfield network, is discussed for Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib55" title="">55</a>]</cite>. The semantic information theoretical measure for LLMs, is discussed from the perspective of directed information estimation. The connection between AR-LLM and other novel architectures, such as Mamba/Mamba2 and LLaDA, are also discussed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib58" title="">58</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p">The rest of this paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2" title="II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">II</span></a> presents the key concepts. In Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3" title="III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">III</span></a>, the LLM is studied as a next-token predictor. Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4" title="IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">IV</span></a> discusses the vector representation of token-level semantics. The general definition of AR-LLMs is proposed in Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5" title="V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">V</span></a>, where the Transformer architecture is thoroughly studied. Other LLM architectures are also discussed in this section. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S6" title="VI Conclusions ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes this paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps">Preliminaries</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">In this section, we will introduce the rate-distortion function, the directed information, and Granger causality, which will play key roles for understanding LLMs in subsequent discussions.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-A </span><span class="ltx_text ltx_font_italic">Rate-distortion Function</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Rate-distortion theory, proposed by Shannon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib1" title="">1</a>]</cite> and systematically discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib43" title="">43</a>]</cite>, addresses the problem of determining the minimum rate <math alttext="R\,\mathrm{bits/symbol}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_unit">R</mi><mo lspace="0.170em" rspace="0em">‚Äã</mo><mi class="ltx_unit">bits</mi></mrow><mo>/</mo><mi class="ltx_unit">symbol</mi></mrow><annotation encoding="application/x-tex">R\,\mathrm{bits/symbol}</annotation></semantics></math>, so that the source symbol can be approximately reconstructed at the receiver without exceeding an expected distortion <math alttext="D" class="ltx_Math" display="inline" id="S2.SS1.p1.m2" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn1">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 1</span></span></h6>
<div class="ltx_para" id="Thmdefn1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The rate-distortion function for a source sequence <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="Thmdefn1.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> with a non-negative distortion measure <math alttext="d" class="ltx_Math" display="inline" id="Thmdefn1.p1.m2" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R(D)=\lim_{n\to\infty}\frac{1}{n}\inf_{P(\hat{X}_{1:n}|X_{1:n}):\mathbb{E}\{d(X_{1:n},\hat{X}_{1:n})\}\leq D}I(X_{1:n};\hat{X}_{1:n})," class="ltx_Math" display="block" id="S2.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">lim</mo><mrow><mi>n</mi><mo stretchy="false">‚Üí</mo><mi mathvariant="normal">‚àû</mi></mrow></munder><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><munder><mo movablelimits="false" rspace="0.167em">inf</mo><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo fence="false">|</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>‚â§</mo><mi>D</mi></mrow></mrow></munder><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R(D)=\lim_{n\to\infty}\frac{1}{n}\inf_{P(\hat{X}_{1:n}|X_{1:n}):\mathbb{E}\{d(X_{1:n},\hat{X}_{1:n})\}\leq D}I(X_{1:n};\hat{X}_{1:n}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\hat{X}_{1:n}" class="ltx_Math" display="inline" id="Thmdefn1.p1.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{1:n}</annotation></semantics></math> is the output of the lossy source codec.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">The rate-distortion function is in general very difficult to compute, where the classical Blahut-Arimoto algorithm is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib60" title="">60</a>]</cite>. Recently, we proposed a communication optimal transport approach and a constrained Blahut-Arimoto algorithm to compute the rate-distortion function and the rate-distortion-perception function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib63" title="">63</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-B </span><span class="ltx_text ltx_font_italic">Directed Information</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">In information theory, the directed information is first defined by Massey in his pioneer work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib42" title="">42</a>]</cite> for discussing the channel with feedback. This idea was systematically developed for extensive channels with feedback in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib64" title="">64</a>]</cite>. Let <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S2.SS2.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> and <math alttext="Y_{1:n}" class="ltx_Math" display="inline" id="S2.SS2.p1.m2" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Y_{1:n}</annotation></semantics></math> be two random sequences with <math alttext="n\in\mathbb{N}" class="ltx_Math" display="inline" id="S2.SS2.p1.m3" intent=":literal"><semantics><mrow><mi>n</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow><annotation encoding="application/x-tex">n\in\mathbb{N}</annotation></semantics></math>, we then have the following definition.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn2">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 2</span></span></h6>
<div class="ltx_para" id="Thmdefn2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The directed information from <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="Thmdefn2.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> to <math alttext="Y_{1:n}" class="ltx_Math" display="inline" id="Thmdefn2.p1.m2" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Y_{1:n}</annotation></semantics></math> is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(X_{1:n}\to Y_{1:n})=\sum_{t=1}^{n}I(X_{1:t};Y_{t}|Y_{1:t-1})." class="ltx_Math" display="block" id="S2.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(X_{1:n}\to Y_{1:n})=\sum_{t=1}^{n}I(X_{1:t};Y_{t}|Y_{1:t-1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">Following this idea, we introduce the backward directed information from <math alttext="X_{n:1}" class="ltx_Math" display="inline" id="S2.SS2.p2.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">X_{n:1}</annotation></semantics></math> to <math alttext="Y_{1:n}" class="ltx_Math" display="inline" id="S2.SS2.p2.m2" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Y_{1:n}</annotation></semantics></math> as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn3">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 3</span></span></h6>
<div class="ltx_para" id="Thmdefn3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The backward directed information from <math alttext="X_{n:1}" class="ltx_Math" display="inline" id="Thmdefn3.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">X_{n:1}</annotation></semantics></math> to <math alttext="Y_{1:n}" class="ltx_Math" display="inline" id="Thmdefn3.p1.m2" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Y_{1:n}</annotation></semantics></math> is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(X_{n:1}\to Y_{1:n})=\sum_{t=1}^{n}I(X_{t+1:n};Y_{t}|Y_{1:t-1})." class="ltx_Math" display="block" id="S2.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(X_{n:1}\to Y_{1:n})=\sum_{t=1}^{n}I(X_{t+1:n};Y_{t}|Y_{1:t-1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">The information density, first proposed by Dobrushin in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib65" title="">65</a>]</cite>, has been widely used in finite blocklength information theory and machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib35" title="">35</a>]</cite>. Similarly, we introduce the directed information density.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn4">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 4</span></span></h6>
<div class="ltx_para" id="Thmdefn4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The directed information density from <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="Thmdefn4.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> to <math alttext="Y_{1:n}" class="ltx_Math" display="inline" id="Thmdefn4.p1.m2" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Y_{1:n}</annotation></semantics></math> is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(X_{1:n}\to Y_{1:n})=\sum_{t=1}^{n}\imath(X_{1:n};Y_{t}|Y_{1:t-1})," class="ltx_Math" display="block" id="S2.E4.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\imath(X_{1:n}\to Y_{1:n})=\sum_{t=1}^{n}\imath(X_{1:n};Y_{t}|Y_{1:t-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(X_{1:n};Y_{t}|Y_{1:t-1})=\log\frac{P(Y_{t}|Y_{1:t-1},X_{1:n})}{P(Y_{t}|Y_{1:t-1})}." class="ltx_Math" display="block" id="S2.E5.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\imath(X_{1:n};Y_{t}|Y_{1:t-1})=\log\frac{P(Y_{t}|Y_{1:t-1},X_{1:n})}{P(Y_{t}|Y_{1:t-1})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p">Similar to the rate-distortion function, it is also very difficult to compute directed information in practice. The classical Blahut-Arimoto algorithm has been extended to maximize directed information in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib66" title="">66</a>]</cite>. Inspired by the idea of mutual information neural estimator (MINE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib67" title="">67</a>]</cite>, the directed information neural estimator (DINE) is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib68" title="">68</a>]</cite>. A seminal work of computing information density is proposed by Strassen in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib69" title="">69</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-C </span><span class="ltx_text ltx_font_italic">Granger Causality</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">Granger, the Nobel prize winner of 2003, proposed a general definition of causality in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib45" title="">45</a>]</cite>, which is referred to as Granger causality afterwards.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn5">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 5</span></span></h6>
<div class="ltx_para" id="Thmdefn5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\mathcal{U}_{t}" class="ltx_Math" display="inline" id="Thmdefn5.p1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ùí∞</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{U}_{t}</annotation></semantics></math> be all the knowledge in the universe available at time <math alttext="t" class="ltx_Math" display="inline" id="Thmdefn5.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> with <math alttext="1\leq t\leq n" class="ltx_Math" display="inline" id="Thmdefn5.p1.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚â§</mo><mi>t</mi><mo>‚â§</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">1\leq t\leq n</annotation></semantics></math>, <math alttext="\mathcal{U}_{t}^{-}" class="ltx_Math" display="inline" id="Thmdefn5.p1.m4" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ùí∞</mi><mi>t</mi><mo>‚àí</mo></msubsup><annotation encoding="application/x-tex">\mathcal{U}_{t}^{-}</annotation></semantics></math> be the knowledge in the modified universe in which <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="Thmdefn5.p1.m5" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> is excluded, <math alttext="X_{t}" class="ltx_Math" display="inline" id="Thmdefn5.p1.m6" intent=":literal"><semantics><msub><mi>X</mi><mi>t</mi></msub><annotation encoding="application/x-tex">X_{t}</annotation></semantics></math> is said to cause <math alttext="Y_{t+1}" class="ltx_Math" display="inline" id="Thmdefn5.p1.m7" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">Y_{t+1}</annotation></semantics></math> if</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(Y_{t+1}\in\mathcal{A}|\mathcal{U}_{t})\neq P(Y_{t+1}\in\mathcal{A}|\mathcal{U}_{t}^{-})." class="ltx_Math" display="block" id="S2.E6.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Y</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚àà</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ùí∞</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚â†</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Y</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚àà</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo fence="false">|</mo><msubsup><mi class="ltx_font_mathcaligraphic">ùí∞</mi><mi>t</mi><mo>‚àí</mo></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P(Y_{t+1}\in\mathcal{A}|\mathcal{U}_{t})\neq P(Y_{t+1}\in\mathcal{A}|\mathcal{U}_{t}^{-}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p">This definition is general but not operational. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib70" title="">70</a>]</cite>, several version of operational definition have been discussed, where the directed information or transfer entropy are proposed as a strength measure of Granger causality. As a finite length version of directed information, the transfer entropy is first introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib71" title="">71</a>]</cite>. In many following works, Granger causality is shown to be equivalent to directed information or transfer entropy for Gaussian vector autoregression (VAR) processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib72" title="">72</a>]</cite>. In fact, Massey also discussed the causality for communication system with feedback in his seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p">The directed information, transfer entropy, and Granger causality are widely used in physics, neuroscience, social networks, and finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib73" title="">73</a>]</cite>. From the perspective of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib74" title="">74</a>]</cite>, however, Granger causality is classified as statistical rather than causal.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps">LLM as a Next-token Predictor</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">Inspired from information theory, this section will introduce the probabilistic model and architecture irrelevant properties for LLMs.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-A </span><span class="ltx_text ltx_font_italic">Probabilistic Model of LLMs</span>
</h3>
<figure class="ltx_figure" id="S3.F1"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="118.68" id="S3.F1.pic1" overflow="visible" version="1.1" viewbox="0 0 552.29 118.68" width="552.29"><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,118.68) matrix(1 0 0 -1 0 0) translate(276.14,0) translate(0,68.55)"><g stroke-width="0.8pt"><path d="M -275.59 0 L -203.33 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -203.33 0)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -250.59 7.24)"><foreignobject height="11.53" overflow="visible" style="--fo_width :2.08em;--fo_height:0.68em;--fo_depth :0.15em;" transform="matrix(1 0 0 -1 0 9.46)" width="28.73"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S3.F1.pic1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M -196.85 -23.62 M -196.85 -23.62 L -196.85 23.62 L -157.48 23.62 L -157.48 -23.62 Z M -157.48 23.62" style="fill:none"></path></g><g stroke-width="0.4pt"><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -181.3 -3.46)"><foreignobject height="12.3" overflow="visible" style="--fo_width :0.6em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="8.26"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="f" class="ltx_Math" display="inline" id="S3.F1.pic1.m2" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -211.47 35.91)"><foreignobject height="12.3" overflow="visible" style="--fo_width :4.96em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="68.61"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Embedding</span></span></foreignobject></g><g stroke-width="0.8pt"><path d="M -157.48 0 L -104.9 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -104.9 0)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -140.68 7.24)"><foreignobject height="11.53" overflow="visible" style="--fo_width :1.84em;--fo_height:0.68em;--fo_depth :0.15em;" transform="matrix(1 0 0 -1 0 9.46)" width="25.46"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.F1.pic1.m3" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M -98.43 -23.62 M -98.43 -23.62 L -98.43 23.62 L 98.43 23.62 L 98.43 -23.62 Z M 98.43 23.62" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -70.56 -3.46)"><foreignobject height="13.84" overflow="visible" style="--fo_width :10.2em;--fo_height:0.75em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 10.38)" width="141.12"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="P(U_{t}|S_{1:n},U_{n+1:t-1};\Phi)" class="ltx_Math" display="inline" id="S3.F1.pic1.m4" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(U_{t}|S_{1:n},U_{n+1:t-1};\Phi)</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -14.99 34.64)"><foreignobject height="9.46" overflow="visible" style="--fo_width :2.17em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="29.98"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">LLM</span></span></foreignobject></g><g stroke-width="0.8pt"><path d="M 98.43 0 L 151 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 151 0)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 120.04 7.24)"><foreignobject height="11.53" overflow="visible" style="--fo_width :1.14em;--fo_height:0.68em;--fo_depth :0.15em;" transform="matrix(1 0 0 -1 0 9.46)" width="15.83"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="U_{t}" class="ltx_Math" display="inline" id="S3.F1.pic1.m5" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M 127.95 0 L 127.95 -47.24" style="fill:none"></path></g><g stroke-width="0.8pt"><path d="M 127.95 -47.24 L 0 -47.24" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 56.06 -61.86)"><foreignobject height="11.53" overflow="visible" style="--fo_width :1.14em;--fo_height:0.68em;--fo_depth :0.15em;" transform="matrix(1 0 0 -1 0 9.46)" width="15.83"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="U_{t}" class="ltx_Math" display="inline" id="S3.F1.pic1.m6" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M 0 -47.24 L 0 -30.1" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 -30.1)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 157.48 -23.62 M 157.48 -23.62 L 157.48 23.62 L 196.85 23.62 L 196.85 -23.62 Z M 196.85 23.62" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 172.64 -1.63)"><foreignobject height="8.65" overflow="visible" style="--fo_width :0.65em;--fo_height:0.43em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 5.96)" width="9.05"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="\varphi" class="ltx_Math" display="inline" id="S3.F1.pic1.m7" intent=":literal"><semantics><mi>œÜ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M 196.85 0 L 269.11 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 269.11 0)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 228.23 7.24)"><foreignobject height="11.53" overflow="visible" style="--fo_width :1.15em;--fo_height:0.68em;--fo_depth :0.15em;" transform="matrix(1 0 0 -1 0 9.46)" width="15.98"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="Y_{t}" class="ltx_Math" display="inline" id="S3.F1.pic1.m8" intent=":literal"><semantics><msub><mi>Y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Y_{t}</annotation></semantics></math></span></span></foreignobject></g></g></g></svg></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The probabilistic model of an LLM at time <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="S3.F1.m8" intent=":literal"><semantics><mrow><mi>t</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>, where <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S3.F1.m9" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> is the token sequence with <math alttext="1\leq n&lt;t\leq T" class="ltx_Math" display="inline" id="S3.F1.m10" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚â§</mo><mi>n</mi><mo>&lt;</mo><mi>t</mi><mo>‚â§</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">1\leq n&lt;t\leq T</annotation></semantics></math> whose semantic vector embedding is <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.F1.m11" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math>, <math alttext="Y_{n+1:T}" class="ltx_Math" display="inline" id="S3.F1.m12" intent=":literal"><semantics><msub><mi>Y</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">Y_{n+1:T}</annotation></semantics></math> is the output token sequence, whose semantic vector embedding is <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.F1.m13" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math>. <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.F1.m14" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math> represents the parameters after training.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">The probabilistic model of LLMs is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.F1" title="Figure 1 ‚Ä£ III-A Probabilistic Model of LLMs ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>. The input token sequence is <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> with <math alttext="1\leq n&lt;t\leq T" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚â§</mo><mi>n</mi><mo>&lt;</mo><mi>t</mi><mo>‚â§</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">1\leq n&lt;t\leq T</annotation></semantics></math> and <math alttext="n\in\mathbb{N}" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>n</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow><annotation encoding="application/x-tex">n\in\mathbb{N}</annotation></semantics></math>, which will be mapped to semantic vector sequence <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> by a semantic embedding module <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. The LLM is modeled as a transition probability with a parameter <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>, which represents the parameters of the LLM after training. The LLM generates the embedding of next token <math alttext="U_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math> based on <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> and the previously generated <math alttext="U_{n+1:t-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.m9" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><annotation encoding="application/x-tex">U_{n+1:t-1}</annotation></semantics></math>, that is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi)." class="ltx_Math" display="block" id="S3.E7.m1" intent=":literal"><semantics><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><math alttext="\varphi" class="ltx_Math" display="inline" id="S3.SS1.p1.m10" intent=":literal"><semantics><mi>œÜ</mi><annotation encoding="application/x-tex">\varphi</annotation></semantics></math> is an inverse module of embedding, which maps <math alttext="U_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m11" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math> to the output token <math alttext="Y_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m12" intent=":literal"><semantics><msub><mi>Y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Y_{t}</annotation></semantics></math>. It should be noticed that the probabilistic model in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.F1" title="Figure 1 ‚Ä£ III-A Probabilistic Model of LLMs ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> is general and architecture irrelevant.</p>
</div>
<div class="ltx_theorem ltx_theorem_rmk" id="Thmrmk1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 1</span></span><span class="ltx_text ltx_font_bold"> (Kolmogorov Complexity Formulation of LLMs)</span>
</h6>
<div class="ltx_para" id="Thmrmk1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The Kolmogorov complexity <math alttext="K(\mathbf{y})" class="ltx_Math" display="inline" id="Thmrmk1.p1.m1" intent=":literal"><semantics><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê≤</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">K(\mathbf{y})</annotation></semantics></math> is defined as the length of the shortest program that generates the output <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m2" intent=":literal"><semantics><mi>ùê≤</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>, formally written as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K(\mathbf{y})=\min_{\mathbf{p}}\{l(\mathbf{p}):T(\mathbf{p})=\mathbf{y}\}," class="ltx_Math" display="block" id="S3.E8.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê≤</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mi>ùê©</mi></munder><mo>‚Å°</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê©</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>T</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê©</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>ùê≤</mi></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">K(\mathbf{y})=\min_{\mathbf{p}}\{l(\mathbf{p}):T(\mathbf{p})=\mathbf{y}\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="T" class="ltx_Math" display="inline" id="Thmrmk1.p1.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is a universal Turing machine, <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m4" intent=":literal"><semantics><mi>ùê©</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> is the program, and <math alttext="l(\mathbf{p})" class="ltx_Math" display="inline" id="Thmrmk1.p1.m5" intent=":literal"><semantics><mrow><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê©</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">l(\mathbf{p})</annotation></semantics></math> is the length of <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m6" intent=":literal"><semantics><mi>ùê©</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math>. According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib75" title="">75</a>]</cite>, the Kolmogorov complexity can be rewritten as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K(\mathbf{y})=\min_{i,\mathbf{p}}\{K(i)+l(\mathbf{p}):T_{i}(\mathbf{p})=\mathbf{y}\}" class="ltx_Math" display="block" id="S3.E9.m1" intent=":literal"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê≤</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>i</mi><mo>,</mo><mi>ùê©</mi></mrow></munder><mo>‚Å°</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê©</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><msub><mi>T</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê©</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>ùê≤</mi></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">K(\mathbf{y})=\min_{i,\mathbf{p}}\{K(i)+l(\mathbf{p}):T_{i}(\mathbf{p})=\mathbf{y}\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="i\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m7" intent=":literal"><semantics><mrow><mi>i</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow><annotation encoding="application/x-tex">i\in\mathbb{N}</annotation></semantics></math> is the index of a sequence of Turing machines. It can be seen that <math alttext="K(\mathbf{y})" class="ltx_Math" display="inline" id="Thmrmk1.p1.m8" intent=":literal"><semantics><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê≤</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">K(\mathbf{y})</annotation></semantics></math> is decomposed into two parts: the first part is a Turing machine <math alttext="T_{i}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m9" intent=":literal"><semantics><msub><mi>T</mi><mi>i</mi></msub><annotation encoding="application/x-tex">T_{i}</annotation></semantics></math>, i.e., the meaningful information or model in the data, and the second part is the irregular aspects of <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m10" intent=":literal"><semantics><mi>ùê≤</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>, i.e., a program <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m11" intent=":literal"><semantics><mi>ùê©</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> to be interpreted by <math alttext="T_{i}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m12" intent=":literal"><semantics><msub><mi>T</mi><mi>i</mi></msub><annotation encoding="application/x-tex">T_{i}</annotation></semantics></math>. Following this idea, the LLM is equivalent to <math alttext="T_{i}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m13" intent=":literal"><semantics><msub><mi>T</mi><mi>i</mi></msub><annotation encoding="application/x-tex">T_{i}</annotation></semantics></math>, <math alttext="\mathbf{p}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m14" intent=":literal"><semantics><mi>ùê©</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> is the input <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="Thmrmk1.p1.m15" intent=":literal"><semantics><mi>ùê±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, i.e., the prompt.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-B </span><span class="ltx_text ltx_font_italic">Directed Rate-distortion Function in Pre-training Phase</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">From the perspective of information theory, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.E7" title="In III-A Probabilistic Model of LLMs ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">7</span></a>) is a discrete-time channel with feedback and state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib64" title="">64</a>]</cite>. The input is <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math>, the output is <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math>, the feedback at time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p1.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is <math alttext="U_{t}" class="ltx_Math" display="inline" id="S3.SS2.p1.m4" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math>, and the channel state is the parameter <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS2.p1.m5" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>. In contrast to the reliable communication problem, the goal here is to ensure the output token sequence aligns with our expectations, rather than a flawless recovery of the input.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">As a discrete-time channel with feedback and state, the directed information is a natural choice to measure the information transferred from <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> to <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> with parameter <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS2.p2.m3" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib64" title="">64</a>]</cite>. According to Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn2" title="Definition 2 ‚Ä£ II-B Directed Information ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(S_{1:n}\to U_{n+1:T};\Phi)=\sum_{t=n+1}^{T}I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi)." class="ltx_Math" display="block" id="S3.E10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(S_{1:n}\to U_{n+1:T};\Phi)=\sum_{t=n+1}^{T}I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let <math alttext="U_{n+1:T}^{\hbar}" class="ltx_Math" display="inline" id="S3.SS2.p2.m4" intent=":literal"><semantics><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">U_{n+1:T}^{\hbar}</annotation></semantics></math> be the labeled sequence by human being with the input <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS2.p2.m5" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math>, and <math alttext="D_{KL}(\cdot\|\cdot)" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.m6" intent=":literal"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo lspace="0em" rspace="0em">‚à•</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">D_{KL}(\cdot\|\cdot)</annotation></semantics></math> be the KL divergence. Denote <math alttext="P_{t}^{\hbar}=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})" class="ltx_Math" display="inline" id="S3.SS2.p2.m7" intent=":literal"><semantics><mrow><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P_{t}^{\hbar}=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})</annotation></semantics></math> and <math alttext="Q_{t}^{\Phi}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi)" class="ltx_Math" display="inline" id="S3.SS2.p2.m8" intent=":literal"><semantics><mrow><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{t}^{\Phi}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi)</annotation></semantics></math> for <math alttext="t=n+1,\ldots,T" class="ltx_Math" display="inline" id="S3.SS2.p2.m9" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=n+1,\ldots,T</annotation></semantics></math>, we then have the following definition.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn6">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 6</span></span></h6>
<div class="ltx_para" id="Thmdefn6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The directed rate-distortion function for LLMs in the pre-training phase is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{pre}(D)=\frac{1}{T}\inf_{\Phi:\frac{1}{T}\sum_{t=n+1}^{T}D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi})&lt;D}I(S_{1:n}\to U_{n+1:T};\Phi)." class="ltx_Math" display="block" id="S3.E11.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><munder><mo movablelimits="false" rspace="0.167em">inf</mo><mrow><mi mathvariant="normal">Œ¶</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mstyle displaystyle="false"><msubsup><mo>‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></msubsup></mstyle><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>‚à•</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>&lt;</mo><mi>D</mi></mrow></mrow></munder><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{pre}(D)=\frac{1}{T}\inf_{\Phi:\frac{1}{T}\sum_{t=n+1}^{T}D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi})&lt;D}I(S_{1:n}\to U_{n+1:T};\Phi).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">Similar to Shannon capacity, <math alttext="R_{pre}(D)" class="ltx_Math" display="inline" id="S3.SS2.p3.m1" intent=":literal"><semantics><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{pre}(D)</annotation></semantics></math> is defined as a universal measure connecting the input sequence <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS2.p3.m2" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> and output sequence <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS2.p3.m3" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math>. Furthermore, <math alttext="R_{pre}(D)" class="ltx_Math" display="inline" id="S3.SS2.p3.m4" intent=":literal"><semantics><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{pre}(D)</annotation></semantics></math> is independent of any implementation methods, such as Transformer or novel architectures yet to be conceived. In contrast to the classical rate-distortion function in Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn1" title="Definition 1 ‚Ä£ II-A Rate-distortion Function ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>, which governs a lossy source codecs, the output sequence <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS2.p3.m5" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> in this context is instead constrained by a condition defined in terms of KL divergence. Therefore, <math alttext="R_{pre}(D)" class="ltx_Math" display="inline" id="S3.SS2.p3.m6" intent=":literal"><semantics><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{pre}(D)</annotation></semantics></math> is the minimum information needed from <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS2.p3.m7" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> to generate the expected <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS2.p3.m8" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> with an average distortion <math alttext="D" class="ltx_Math" display="inline" id="S3.SS2.p3.m9" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>. The curve of <math alttext="R_{pre}(D)" class="ltx_Math" display="inline" id="S3.SS2.p3.m10" intent=":literal"><semantics><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{pre}(D)</annotation></semantics></math> versus the optimization process of <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS2.p3.m11" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math> will reveal key properties of the pre-training in practice. Simple derivation will give us the following theorem.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm1">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 1</span></span></h6>
<div class="ltx_para" id="Thmthm1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In the pre-training phase with the cross-entropy loss, we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{pre}(0)=\frac{1}{T}I(S_{1:n}\to U_{n+1:T}^{\hbar})," class="ltx_Math" display="block" id="S3.E12.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R_{pre}(0)=\frac{1}{T}I(S_{1:n}\to U_{n+1:T}^{\hbar}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">when convergence.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p">The cross-entropy between <math alttext="P_{t}^{\hbar}" class="ltx_Math" display="inline" id="S3.SS2.p4.m1" intent=":literal"><semantics><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">P_{t}^{\hbar}</annotation></semantics></math> and <math alttext="Q_{t}^{\Phi}" class="ltx_Math" display="inline" id="S3.SS2.p4.m2" intent=":literal"><semantics><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><annotation encoding="application/x-tex">Q_{t}^{\Phi}</annotation></semantics></math> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(P_{t}^{\hbar},Q_{t}^{\Phi})=H(P_{t}^{\hbar})+D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi}),\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S3.E13.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>‚à•</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(P_{t}^{\hbar},Q_{t}^{\Phi})=H(P_{t}^{\hbar})+D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi}),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, the objective of pre-training can be written as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\Phi}H(P_{t}^{\hbar},Q_{t}^{\Phi})\Leftrightarrow\min_{\Phi}D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi}),\quad t=n+1,\ldots,T." class="ltx_math_unparsed" display="block" id="S3.E14.m1" intent=":literal"><semantics><mrow><munder><mi>min</mi><mi mathvariant="normal">Œ¶</mi></munder><mi>H</mi><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">‚áî</mo><munder><mi>min</mi><mi mathvariant="normal">Œ¶</mi></munder><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo lspace="0em" rspace="0.167em">‚à•</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo stretchy="false">)</mo></mrow><mo rspace="1.167em">,</mo><mi>t</mi><mo>=</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\Phi}H(P_{t}^{\hbar},Q_{t}^{\Phi})\Leftrightarrow\min_{\Phi}D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi}),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The minimization is achieved by adjusting <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS2.p4.m3" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t}^{\Phi^{\hbar}}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})=P_{t}^{\hbar},\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S3.E15.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>Q</mi><mi>t</mi><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></msubsup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">Q_{t}^{\Phi^{\hbar}}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})=P_{t}^{\hbar},\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\Phi^{\hbar}" class="ltx_Math" display="inline" id="S3.SS2.p4.m4" intent=":literal"><semantics><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup><annotation encoding="application/x-tex">\Phi^{\hbar}</annotation></semantics></math> is the optimal solution of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.E14" title="In III-B Directed Rate-distortion Function in Pre-training Phase ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">14</span></a>). It implies that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="D=D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi^{\hbar}})=0," class="ltx_Math" display="block" id="S3.E16.m1" intent=":literal"><semantics><mrow><mrow><mi>D</mi><mo>=</mo><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>‚à•</mo><msubsup><mi>Q</mi><mi>t</mi><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">D=D_{KL}(P_{t}^{\hbar}\|Q_{t}^{\Phi^{\hbar}})=0,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">when convergence. Recalling Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn2" title="Definition 2 ‚Ä£ II-B Directed Information ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>, we have</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E17">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E17X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar})=\sum_{t=n+1}^{T}I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar})" class="ltx_Math" display="inline" id="S3.E17X.m2" intent=":literal"><semantics><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover></mstyle><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar})=\sum_{t=n+1}^{T}I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="4"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(17)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E17Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E17Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sum_{t=n+1}^{T}H(U_{t}|U_{n+1:t-1};\Phi^{\hbar})-\sum_{t=n+1}^{T}H(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})" class="ltx_Math" display="inline" id="S3.E17Xa.m3" intent=":literal"><semantics><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover></mstyle><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‚àí</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover></mstyle><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{t=n+1}^{T}H(U_{t}|U_{n+1:t-1};\Phi^{\hbar})-\sum_{t=n+1}^{T}H(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E17Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E17Xb.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sum_{t=n+1}^{T}H(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar})-\sum_{t=n+1}^{T}H(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})" class="ltx_Math" display="inline" id="S3.E17Xb.m3" intent=":literal"><semantics><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover></mstyle><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo fence="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‚àí</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>T</mi></munderover></mstyle><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{t=n+1}^{T}H(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar})-\sum_{t=n+1}^{T}H(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E17Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E17Xc.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle I(S_{1:n}\to U_{n+1:T}^{\hbar})." class="ltx_Math" display="inline" id="S3.E17Xc.m3" intent=":literal"><semantics><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle I(S_{1:n}\to U_{n+1:T}^{\hbar}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">This theorem has been established.
‚àé</p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p">The aforementioned definition and theorem show that minimizing the directed information by adjusting <math alttext="\Phi" class="ltx_Math" display="inline" id="S3.SS2.p5.m1" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math> filters out information irrelevant to generate the output, which may effectively prevent hallucinations caused by the propagation of extraneous information by LLMs. Therefore, we suggest to use the following loss function for LLM pre-training:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\Phi)=I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi)+\lambda H(P_{t}^{\hbar},Q_{t}^{\Phi}),\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S3.E18.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Œª</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\Phi)=I(S_{1:n};U_{t}|U_{n+1:t-1};\Phi)+\lambda H(P_{t}^{\hbar},Q_{t}^{\Phi}),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p5.m2" intent=":literal"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is the Lagrangian multiplier.</p>
</div>
<div class="ltx_theorem ltx_theorem_rmk" id="Thmrmk2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 2</span></span><span class="ltx_text ltx_font_bold"> (Information Geometry and Pre-training)</span>
</h6>
<div class="ltx_para" id="Thmrmk2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Consider the pre-training phase, the distribution before and after one training step is denoted by <math alttext="P_{t}(\Phi)=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi)" class="ltx_Math" display="inline" id="Thmrmk2.p1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P_{t}(\Phi)=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi)</annotation></semantics></math> and <math alttext="P_{t}(\Phi^{\prime})=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\prime})" class="ltx_Math" display="inline" id="Thmrmk2.p1.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Œ¶</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mo>‚Ä≤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P_{t}(\Phi^{\prime})=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\prime})</annotation></semantics></math>, respectively. According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib76" title="">76</a>]</cite>, the entry of the Fisher information matrix at the <math alttext="i" class="ltx_Math" display="inline" id="Thmrmk2.p1.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th row and <math alttext="j" class="ltx_Math" display="inline" id="Thmrmk2.p1.m4" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th column is given by</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="[\mathcal{I}(\Phi)]_{ij}=\left.\frac{\partial^{2}}{\partial\Phi^{\prime}_{i}\partial\Phi^{\prime}_{j}}H(P_{t}(\Phi),P_{t}(\Phi^{\prime}))\right|_{\Phi^{\prime}=\Phi}." class="ltx_Math" display="block" id="S3.E19.m1" intent=":literal"><semantics><mrow><mrow><msub><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">‚Ñê</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mrow><mrow><mfrac><msup><mo>‚àÇ</mo><mn>2</mn></msup><mrow><mo rspace="0em">‚àÇ</mo><mrow><msubsup><mi mathvariant="normal">Œ¶</mi><mi>i</mi><mo>‚Ä≤</mo></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo rspace="0em">‚àÇ</mo><msubsup><mi mathvariant="normal">Œ¶</mi><mi>j</mi><mo>‚Ä≤</mo></msubsup></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Œ¶</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>|</mo></mrow><mrow><msup><mi mathvariant="normal">Œ¶</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi mathvariant="normal">Œ¶</mi></mrow></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">[\mathcal{I}(\Phi)]_{ij}=\left.\frac{\partial^{2}}{\partial\Phi^{\prime}_{i}\partial\Phi^{\prime}_{j}}H(P_{t}(\Phi),P_{t}(\Phi^{\prime}))\right|_{\Phi^{\prime}=\Phi}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Thus, the Fisher information matrix represents the curvature of the cross-entropy loss with respect to the parameters <math alttext="\Phi" class="ltx_Math" display="inline" id="Thmrmk2.p1.m5" intent=":literal"><semantics><mi mathvariant="normal">Œ¶</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>. By modifying the gradient with Fisher information matrix, the natural gradient method is then proposed for neural network training. Due to the high computation complexity and storage cost, the Kronecker-factored approximate curvature method is used in practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib77" title="">77</a>]</cite>.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-C </span><span class="ltx_text ltx_font_italic">Directed Rate-reward Function in Post-training Phase</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">The objective of pre-training is to accurately predict the next-token. The generated token sequence, however, may not follow the human preference. The post-training shifts the focus to evaluate whether the entire generated sequence aligns with human preferences by fine-tuning with reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">An evaluation function <math alttext="w(S_{1:n},U_{n+1:T})" class="ltx_Math" display="inline" id="S3.SS3.p2.m1" intent=":literal"><semantics><mrow><mi>w</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">w(S_{1:n},U_{n+1:T})</annotation></semantics></math>, the reward function in RLHF, is introduced to assign a score to the generated sequence <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS3.p2.m2" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> for the input sequence <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS3.p2.m3" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math>. We then have the following definition.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn7">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 7</span></span></h6>
<div class="ltx_para" id="Thmdefn7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The directed rate-reward function for LLMs in the post-training phase is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{post}(W)=\frac{1}{T}\inf_{\Phi^{\hbar}:w(S_{1:n},U_{n+1:T})&gt;W}I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar})." class="ltx_Math" display="block" id="S3.E20.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><munder><mo movablelimits="false" rspace="0.167em">inf</mo><mrow><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi>w</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mi>W</mi></mrow></mrow></munder><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{post}(W)=\frac{1}{T}\inf_{\Phi^{\hbar}:w(S_{1:n},U_{n+1:T})&gt;W}I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(20)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p">Therefore, we suggest to use the following loss function for LLM post-training:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\Phi^{\hbar})=I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar})-\lambda w(S_{1:n},U_{n+1:T})," class="ltx_Math" display="block" id="S3.E21.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚àí</mo><mrow><mi>Œª</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>w</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\Phi^{\hbar})=I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar})-\lambda w(S_{1:n},U_{n+1:T}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS3.p3.m1" intent=":literal"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is the Lagrangian multiplier. The optimization solution will be denoted as <math alttext="\Phi^{\hbar+}" class="ltx_Math" display="inline" id="S3.SS3.p3.m2" intent=":literal"><semantics><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><annotation encoding="application/x-tex">\Phi^{\hbar+}</annotation></semantics></math>. Recalling the proof of Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm1" title="Theorem 1 ‚Ä£ III-B Directed Rate-distortion Function in Pre-training Phase ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>, <math alttext="\mathcal{L}(\Phi^{\hbar})" class="ltx_Math" display="inline" id="S3.SS3.p3.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\Phi^{\hbar})</annotation></semantics></math> is equivalent to the loss function of RL fine-tuning phase in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib78" title="">78</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p">Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm1" title="Theorem 1 ‚Ä£ III-B Directed Rate-distortion Function in Pre-training Phase ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> shows that the LLM approaches <math alttext="I(S_{1:n}\to U_{n+1:T}^{\hbar})" class="ltx_Math" display="inline" id="S3.SS3.p4.m1" intent=":literal"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(S_{1:n}\to U_{n+1:T}^{\hbar})</annotation></semantics></math> during pre-training, which measures the information transferred from <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS3.p4.m2" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> to <math alttext="U_{n+1:T}^{\hbar}" class="ltx_Math" display="inline" id="S3.SS3.p4.m3" intent=":literal"><semantics><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">U_{n+1:T}^{\hbar}</annotation></semantics></math> by human being. The post-training further adjusts the parameter from <math alttext="\Phi^{\hbar}" class="ltx_Math" display="inline" id="S3.SS3.p4.m4" intent=":literal"><semantics><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup><annotation encoding="application/x-tex">\Phi^{\hbar}</annotation></semantics></math> to <math alttext="\Phi^{\hbar+}" class="ltx_Math" display="inline" id="S3.SS3.p4.m5" intent=":literal"><semantics><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><annotation encoding="application/x-tex">\Phi^{\hbar+}</annotation></semantics></math> such that the generated sequence <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS3.p4.m6" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> meets human preferences. Recalling the discussion in Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S2.SS3" title="II-C Granger Causality ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">II-C</span></a>, we have the following conclusion.</p>
</div>
<div class="ltx_theorem ltx_theorem_cor" id="Thmcor1">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 1</span></span></h6>
<div class="ltx_para" id="Thmcor1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The LLM approaches the human-level Granger causality for next-token prediction with human preference after training.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-D </span><span class="ltx_text ltx_font_italic">Semantic Information Flow in Inference Phase</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">During the inference phase, the LLM with parameter <math alttext="\Phi^{\hbar+}" class="ltx_Math" display="inline" id="S3.SS4.p1.m1" intent=":literal"><semantics><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><annotation encoding="application/x-tex">\Phi^{\hbar+}</annotation></semantics></math> is employed to generate the output token sequence <math alttext="U_{n+1:T}" class="ltx_Math" display="inline" id="S3.SS4.p1.m2" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:T}</annotation></semantics></math> based on the input token sequence <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S3.SS4.p1.m3" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math>. In contrast to the post-training phase, where the focus is on the average performance across all possible output sequences, the inference phase considers the specific output sequence for the given input sequence. Therefore, it is natural to use the directed information density in Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn4" title="Definition 4 ‚Ä£ II-B Directed Information ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> to analyze the inference process. The semantic information flow can then be defined as follows.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn8">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 8</span></span></h6>
<div class="ltx_para" id="Thmdefn8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The semantic information flow for LLMs is defined as the directed information density from <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="Thmdefn8.p1.m1" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> to <math alttext="U_{n+1:t}" class="ltx_Math" display="inline" id="Thmdefn8.p1.m2" intent=":literal"><semantics><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">U_{n+1:t}</annotation></semantics></math> as follows:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=\sum_{\tau=n+1}^{t}\imath(S_{1:n};U_{\tau}|U_{n+1:\tau-1};\Phi^{\hbar+}),\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S3.E22.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>œÑ</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>t</mi></munderover><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>œÑ</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>œÑ</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=\sum_{\tau=n+1}^{t}\imath(S_{1:n};U_{\tau}|U_{n+1:\tau-1};\Phi^{\hbar+}),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(22)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p">In the inference phase, the generation will stop when a special token, denoted by <math alttext="\lhd" class="ltx_Math" display="inline" id="S3.SS4.p2.m1" intent=":literal"><semantics><mo>‚ä≤</mo><annotation encoding="application/x-tex">\lhd</annotation></semantics></math>, is generated. Thus, <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the stopping time with respect to the event <math alttext="\{U_{T}=\mathbf{s}(\lhd)\}" class="ltx_Math" display="inline" id="S3.SS4.p2.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mrow><msub><mi>U</mi><mi>T</mi></msub><mo>=</mo><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ä≤</mo><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{U_{T}=\mathbf{s}(\lhd)\}</annotation></semantics></math>, where the vector representation of <math alttext="\lhd" class="ltx_Math" display="inline" id="S3.SS4.p2.m4" intent=":literal"><semantics><mo>‚ä≤</mo><annotation encoding="application/x-tex">\lhd</annotation></semantics></math> is <math alttext="\mathbf{s}(\lhd)" class="ltx_Math" display="inline" id="S3.SS4.p2.m5" intent=":literal"><semantics><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ä≤</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{s}(\lhd)</annotation></semantics></math>. We then have the following theorem.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm2">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2</span></span></h6>
<div class="ltx_para" id="Thmthm2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The semantic information flow <math alttext="\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})" class="ltx_Math" display="inline" id="Thmthm2.p1.m1" intent=":literal"><semantics><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})</annotation></semantics></math> is a Markovian sub-martingale for <math alttext="t=n+1,\ldots,T" class="ltx_Math" display="inline" id="Thmthm2.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=n+1,\ldots,T</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p">According to the Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn4" title="Definition 4 ‚Ä£ II-B Directed Information ‚Ä£ II Preliminaries ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">4</span></a>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+})," class="ltx_Math" display="block" id="S3.E23.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+})=\log\frac{P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{P(U_{t}|U_{n+1:t-1};\Phi^{\hbar+})}." class="ltx_Math" display="block" id="S3.E24.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+})=\log\frac{P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{P(U_{t}|U_{n+1:t-1};\Phi^{\hbar+})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we consider the conditional expectation as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E25">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E25X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+}),\ldots,\imath(S_{1:n}\to U_{n+1};\Phi^{\hbar+})\}" class="ltx_Math" display="inline" id="S3.E25X.m2" intent=":literal"><semantics><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">|</mo><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+}),\ldots,\imath(S_{1:n}\to U_{n+1};\Phi^{\hbar+})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="5"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(25)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E25Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E25Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})\}" class="ltx_Math" display="inline" id="S3.E25Xa.m3" intent=":literal"><semantics><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">|</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E25Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E25Xb.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+\mathbb{E}\{\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+})\}" class="ltx_Math" display="inline" id="S3.E25Xb.m3" intent=":literal"><semantics><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+\mathbb{E}\{\imath(S_{1:n};U_{t}|U_{n+1:t-1};\Phi^{\hbar+})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E25Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E25Xc.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+D_{KL}(P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\|P(U_{t}|U_{n+1:t-1};\Phi^{\hbar+}))" class="ltx_math_unparsed" display="inline" id="S3.E25Xc.m3" intent=":literal"><semantics><mrow><mi>ƒ±</mi><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo>+</mo><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.167em">‚à•</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})+D_{KL}(P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\|P(U_{t}|U_{n+1:t-1};\Phi^{\hbar+}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E25Xd">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\geq" class="ltx_Math" display="inline" id="S3.E25Xd.m2" intent=":literal"><semantics><mo>‚â•</mo><annotation encoding="application/x-tex">\displaystyle\geq</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+})." class="ltx_Math" display="inline" id="S3.E25Xd.m3" intent=":literal"><semantics><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\imath(S_{1:n}\to U_{n+1:t-1};\Phi^{\hbar+}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The last inequality holds because the KL divergence is non-negative, which establishes this theorem.
‚àé</p>
</div>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p">In the following, we will discuss the properties of semantic information flow as a sub-martingale. According to Doob decomposition, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=M_{t}+A_{t}," class="ltx_Math" display="block" id="S3.E26.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>M</mi><mi>t</mi></msub><mo>+</mo><msub><mi>A</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\imath(S_{1:n}\to U_{n+1:t};\Phi^{\hbar+})=M_{t}+A_{t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="A_{t}" class="ltx_Math" display="inline" id="S3.SS4.p4.m1" intent=":literal"><semantics><msub><mi>A</mi><mi>t</mi></msub><annotation encoding="application/x-tex">A_{t}</annotation></semantics></math> is a predictable and non-decreasing process</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{t}=\sum_{j=n+1}^{t}\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:j};\Phi^{\hbar+})-\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})\}" class="ltx_Math" display="block" id="S3.E27.m1" intent=":literal"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>t</mi></munderover><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>j</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚àí</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em" rspace="0em">|</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">A_{t}=\sum_{j=n+1}^{t}\mathbb{E}\{\imath(S_{1:n}\to U_{n+1:j};\Phi^{\hbar+})-\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})|\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="M_{t}" class="ltx_Math" display="inline" id="S3.SS4.p4.m2" intent=":literal"><semantics><msub><mi>M</mi><mi>t</mi></msub><annotation encoding="application/x-tex">M_{t}</annotation></semantics></math> is a martingale</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E28">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E28X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle M_{t}=" class="ltx_Math" display="inline" id="S3.E28X.m2" intent=":literal"><semantics><mrow><msub><mi>M</mi><mi>t</mi></msub><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle M_{t}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\imath(S_{1:n}\to U_{n+1};\Phi^{\hbar+})" class="ltx_Math" display="inline" id="S3.E28X.m3" intent=":literal"><semantics><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\imath(S_{1:n}\to U_{n+1};\Phi^{\hbar+})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(28)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E28Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\sum_{j=n+2}^{t}(\imath(S_{1:n}\to U_{n+1:j};\Phi^{\hbar+})-\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})-A_{j})." class="ltx_Math" display="inline" id="S3.E28Xa.m2" intent=":literal"><semantics><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></mrow><mi>t</mi></munderover></mstyle><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>j</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚àí</mo><mrow><mi>ƒ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚àí</mo><msub><mi>A</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle+\sum_{j=n+2}^{t}(\imath(S_{1:n}\to U_{n+1:j};\Phi^{\hbar+})-\imath(S_{1:n}\to U_{n+1:j-1};\Phi^{\hbar+})-A_{j}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">Define the sum of the conditional variances of the differences as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{t}=\sum_{j=n+1}^{t}\mathbb{E}\{(M_{j}-M_{j-1})^{2}|M_{j-1},\ldots,M_{n+1}\}." class="ltx_Math" display="block" id="S3.E29.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>V</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>t</mi></munderover><mrow><mi>ùîº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>M</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">|</mo><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>M</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">V_{t}=\sum_{j=n+1}^{t}\mathbb{E}\{(M_{j}-M_{j-1})^{2}|M_{j-1},\ldots,M_{n+1}\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The following corollary can be directly established according to Freedman‚Äôs inequality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib79" title="">79</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_cor" id="Thmcor2">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 2</span></span></h6>
<div class="ltx_para" id="Thmcor2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For all <math alttext="\alpha,\beta&gt;0" class="ltx_Math" display="inline" id="Thmcor2.p1.m1" intent=":literal"><semantics><mrow><mrow><mi>Œ±</mi><mo>,</mo><mi>Œ≤</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha,\beta&gt;0</annotation></semantics></math>, we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Pr\{M_{t}&gt;\alpha,V_{t}&lt;\beta\}\leq\exp\left(-\frac{\alpha^{2}}{2(\alpha+\beta)}\right)." class="ltx_Math" display="block" id="S3.E30.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Pr</mi><mo>‚Å°</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>M</mi><mi>t</mi></msub><mo>&gt;</mo><mi>Œ±</mi></mrow><mo>,</mo><mrow><msub><mi>V</mi><mi>t</mi></msub><mo>&lt;</mo><mi>Œ≤</mi></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>‚â§</mo><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo>(</mo><mrow><mo>‚àí</mo><mfrac><msup><mi>Œ±</mi><mn>2</mn></msup><mrow><mn>2</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Œ±</mi><mo>+</mo><mi>Œ≤</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Pr\{M_{t}&gt;\alpha,V_{t}&lt;\beta\}\leq\exp\left(-\frac{\alpha^{2}}{2(\alpha+\beta)}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(30)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p">According to Doob‚Äôs optional stopping time theorem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib80" title="">80</a>]</cite> for sub-martingale, we have the following corollary directly.</p>
</div>
<div class="ltx_theorem ltx_theorem_cor" id="Thmcor3">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 3</span></span></h6>
<div class="ltx_para" id="Thmcor3.p1">
<table class="ltx_equation ltx_eqn_table" id="S3.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar+})\geq I(S_{1:n}\to U_{n+1};\Phi^{\hbar+})." class="ltx_Math" display="block" id="S3.E31.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚â•</mo><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar+})\geq I(S_{1:n}\to U_{n+1};\Phi^{\hbar+}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(31)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p">Sharing the same spirit of Shannon capacity, i.e., the maximum mutual information over all input distributions, this corollary inspired us to give the following definition.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn9">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 9</span></span></h6>
<div class="ltx_para" id="Thmdefn9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The semantic information capacity for LLMs is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{P(S_{1:n}):w(S_{1:n},U_{n+1:T})&gt;W}I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar+})." class="ltx_Math" display="block" id="S3.E32.m1" intent=":literal"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>w</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mi>W</mi></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{P(S_{1:n}):w(S_{1:n},U_{n+1:T})&gt;W}I(S_{1:n}\to U_{n+1:T};\Phi^{\hbar+}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(32)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS4.p7">
<p class="ltx_p">Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3.E32" title="In Definition 9 ‚Ä£ III-D Semantic Information Flow in Inference Phase ‚Ä£ III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">32</span></a>) can be seen as a theoretical foundation for prompt engineering.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps">Vector Representation of Token-level Semantics</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">A prerequisite for the efficient training of LLMs is the effective representation of token-level semantics. This section will first define the token-level semantic space, and then elaborate on the vector representation of semantics, semantic compression/de-dimensionality, and the information-theoretic optimal semantic embedding/vectorization.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-A </span><span class="ltx_text ltx_font_italic">Token-level Semantic Space</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">While grammatical and logical rules are central to how human being communicate and think, they are of indirect utility for the automated and computationally efficient processing of natural language by machines. As a starting point, we will disregard the use of intrinsic grammatical and logical structure of a natural language, considering it solely from a probabilistic standpoint.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn10">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 10</span></span></h6>
<div class="ltx_para" id="Thmdefn10.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The token-level semantic space of a language is a probabilistic space <math alttext="(\Omega,\mathscr{F},P)" class="ltx_Math" display="inline" id="Thmdefn10.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ©</mi><mo>,</mo><mi class="ltx_font_mathscript">‚Ñ±</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\Omega,\mathscr{F},P)</annotation></semantics></math>, where <math alttext="|\Omega|=N\geq 1" class="ltx_Math" display="inline" id="Thmdefn10.p1.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">|</mo></mrow><mo>=</mo><mi>N</mi><mo>‚â•</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|\Omega|=N\geq 1</annotation></semantics></math> is a set of all tokens, each of which is the atomic unit with specific semantics in this language, <math alttext="\mathscr{F}\subseteq 2^{\Omega}" class="ltx_Math" display="inline" id="Thmdefn10.p1.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathscript">‚Ñ±</mi><mo>‚äÜ</mo><msup><mn>2</mn><mi mathvariant="normal">Œ©</mi></msup></mrow><annotation encoding="application/x-tex">\mathscr{F}\subseteq 2^{\Omega}</annotation></semantics></math> is the <math alttext="\sigma" class="ltx_Math" display="inline" id="Thmdefn10.p1.m4" intent=":literal"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>-algebra, <math alttext="P" class="ltx_Math" display="inline" id="Thmdefn10.p1.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is the probability measure defined on <math alttext="\mathscr{F}" class="ltx_Math" display="inline" id="Thmdefn10.p1.m6" intent=":literal"><semantics><mi class="ltx_font_mathscript">‚Ñ±</mi><annotation encoding="application/x-tex">\mathscr{F}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">The probability measure <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.p2.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>, which can be learned from large corpus, encodes semantics of every token in the language with intrinsic grammatical and logical structures. A token sequence generated from <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.p2.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> may not be an understandable sentence for human being, because it may not follow grammatical and logical structures with certain probability. However, computing based directly on the probability measure <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.p2.m3" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is very costly and not practical. Therefore, we need to find a computation efficient representation of token-level semantics.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-B </span><span class="ltx_text ltx_font_italic">Token-level Semantic Vector Space</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">It took decades of effort to finally discover that the crucial step was to transition from token-level probabilistic models to semantic models based on vector representations. The shift is favored for its computational efficiency and its remarkable effectiveness in NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib27" title="">27</a>]</cite>. However, this conclusion is drawn mainly from extensive experiments and lacks a solid theoretical foundation. In this subsection, we will attempt to establish the mathematical foundations of semantic vector spaces.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn11">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 11</span></span></h6>
<div class="ltx_para" id="Thmdefn11.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The token-level semantic vector space of a language is a probabilistic inner product space <math alttext="\mathsf{S}=(\mathbb{S}^{N-1},\mathscr{F},\mu,\langle\cdot,\cdot\rangle)" class="ltx_Math" display="inline" id="Thmdefn11.p1.m1" intent=":literal"><semantics><mrow><mi>ùñ≤</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><mo>,</mo><mi class="ltx_font_mathscript">‚Ñ±</mi><mo>,</mo><mi>Œº</mi><mo>,</mo><mrow><mo stretchy="false">‚ü®</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo stretchy="false">‚ü©</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{S}=(\mathbb{S}^{N-1},\mathscr{F},\mu,\langle\cdot,\cdot\rangle)</annotation></semantics></math>, where <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="Thmdefn11.p1.m2" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math> is a <math alttext="(N-1)" class="ltx_Math" display="inline" id="Thmdefn11.p1.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N-1)</annotation></semantics></math>-dimensional unit sphere, each <math alttext="\mathbf{s}\in\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="Thmdefn11.p1.m4" intent=":literal"><semantics><mrow><mi>ùê¨</mi><mo>‚àà</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{s}\in\mathbb{S}^{N-1}</annotation></semantics></math> represents a semantic vector, <math alttext="\mathscr{F}" class="ltx_Math" display="inline" id="Thmdefn11.p1.m5" intent=":literal"><semantics><mi class="ltx_font_mathscript">‚Ñ±</mi><annotation encoding="application/x-tex">\mathscr{F}</annotation></semantics></math> is a <math alttext="\sigma" class="ltx_Math" display="inline" id="Thmdefn11.p1.m6" intent=":literal"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>-algebra on <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="Thmdefn11.p1.m7" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math>, <math alttext="\mu" class="ltx_Math" display="inline" id="Thmdefn11.p1.m8" intent=":literal"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> is a probability measure defined on <math alttext="\mathscr{F}" class="ltx_Math" display="inline" id="Thmdefn11.p1.m9" intent=":literal"><semantics><mi class="ltx_font_mathscript">‚Ñ±</mi><annotation encoding="application/x-tex">\mathscr{F}</annotation></semantics></math>, <math alttext="\langle\cdot,\cdot\rangle" class="ltx_Math" display="inline" id="Thmdefn11.p1.m10" intent=":literal"><semantics><mrow><mo stretchy="false">‚ü®</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo stretchy="false">‚ü©</mo></mrow><annotation encoding="application/x-tex">\langle\cdot,\cdot\rangle</annotation></semantics></math> is an inner product.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">If we use <math alttext="\mathbf{s_{1}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m1" intent=":literal"><semantics><msub><mi>ùê¨</mi><mn>ùüè</mn></msub><annotation encoding="application/x-tex">\mathbf{s_{1}}</annotation></semantics></math> and <math alttext="\mathbf{s}_{2}" class="ltx_Math" display="inline" id="S4.SS2.p2.m2" intent=":literal"><semantics><msub><mi>ùê¨</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathbf{s}_{2}</annotation></semantics></math> to denote two column vectors on <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS2.p2.m3" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math>, the inner product can be written as <math alttext="\langle\mathbf{s}_{1},\mathbf{s}_{2}\rangle=\mathbf{s}_{1}^{T}\mathbf{s}_{2}" class="ltx_Math" display="inline" id="S4.SS2.p2.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">‚ü®</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ùê¨</mi><mn>2</mn></msub><mo stretchy="false">‚ü©</mo></mrow><mo>=</mo><mrow><msubsup><mi>ùê¨</mi><mn>1</mn><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">\langle\mathbf{s}_{1},\mathbf{s}_{2}\rangle=\mathbf{s}_{1}^{T}\mathbf{s}_{2}</annotation></semantics></math>. The squared Euclidean distance is defined as <math alttext="d_{e}^{2}(\mathbf{s}_{1},\mathbf{s}_{2})=\|\mathbf{s}_{1}-\mathbf{s}_{2}\|^{2}=(\mathbf{s}_{1}-\mathbf{s}_{2})^{T}(\mathbf{s}_{1}-\mathbf{s}_{2})" class="ltx_Math" display="inline" id="S4.SS2.p2.m5" intent=":literal"><semantics><mrow><mrow><msubsup><mi>d</mi><mi>e</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ùê¨</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo stretchy="false">‚Äñ</mo><mrow><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>ùê¨</mi><mn>2</mn></msub></mrow><mo stretchy="false">‚Äñ</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>ùê¨</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>ùê¨</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">d_{e}^{2}(\mathbf{s}_{1},\mathbf{s}_{2})=\|\mathbf{s}_{1}-\mathbf{s}_{2}\|^{2}=(\mathbf{s}_{1}-\mathbf{s}_{2})^{T}(\mathbf{s}_{1}-\mathbf{s}_{2})</annotation></semantics></math>. The cosine similarity is defined as <math alttext="\cos(\mathbf{s}_{1},\mathbf{s}_{2})=\mathbf{s}_{1}^{T}\mathbf{s}_{2}" class="ltx_Math" display="inline" id="S4.SS2.p2.m6" intent=":literal"><semantics><mrow><mrow><mi>cos</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ùê¨</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>ùê¨</mi><mn>1</mn><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">\cos(\mathbf{s}_{1},\mathbf{s}_{2})=\mathbf{s}_{1}^{T}\mathbf{s}_{2}</annotation></semantics></math>. It is noticed that <math alttext="\Omega" class="ltx_Math" display="inline" id="S4.SS2.p2.m7" intent=":literal"><semantics><mi mathvariant="normal">Œ©</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> in Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn10" title="Definition 10 ‚Ä£ IV-A Token-level Semantic Space ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">10</span></a> can only be mapped to <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p2.m8" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> points in <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS2.p2.m9" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math>. Let the set of semantic vector of tokens in <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS2.p2.m10" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ùíú</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> be <math alttext="\mathcal{S}(\mathcal{A})\subset\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS2.p2.m11" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚äÇ</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}(\mathcal{A})\subset\mathbb{S}^{N-1}</annotation></semantics></math> with <math alttext="\forall\mathcal{A}\subseteq\Omega" class="ltx_Math" display="inline" id="S4.SS2.p2.m12" intent=":literal"><semantics><mrow><mrow><mo rspace="0.167em">‚àÄ</mo><mi class="ltx_font_mathcaligraphic">ùíú</mi></mrow><mo>‚äÜ</mo><mi mathvariant="normal">Œ©</mi></mrow><annotation encoding="application/x-tex">\forall\mathcal{A}\subseteq\Omega</annotation></semantics></math>. Thus, <math alttext="\mu" class="ltx_Math" display="inline" id="S4.SS2.p2.m13" intent=":literal"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> is an extension from <math alttext="P" class="ltx_Math" display="inline" id="S4.SS2.p2.m14" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> such that <math alttext="\mu(\mathcal{S}(\mathcal{A}))=P(\mathcal{A})" class="ltx_Math" display="inline" id="S4.SS2.p2.m15" intent=":literal"><semantics><mrow><mrow><mi>Œº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mu(\mathcal{S}(\mathcal{A}))=P(\mathcal{A})</annotation></semantics></math> if <math alttext="\mathcal{A}\in\mathscr{F}_{\mathsf{S}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m16" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo>‚àà</mo><msub><mi class="ltx_font_mathscript">‚Ñ±</mi><mi>ùñ≤</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{A}\in\mathscr{F}_{\mathsf{S}}</annotation></semantics></math>, otherwise <math alttext="\mu(\mathcal{S}(\mathcal{A}))=0" class="ltx_Math" display="inline" id="S4.SS2.p2.m17" intent=":literal"><semantics><mrow><mrow><mi>Œº</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ùíú</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu(\mathcal{S}(\mathcal{A}))=0</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p">Many works suggest that the semantic vector space should be a more complex low dimensional manifold. In practice, however, the Euclidean distance and cosine similarity remain the most widely used metrics, because of its simplicity in computation and adequate performance. Therefore, we argue that defining the semantic vector space directly on <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS2.p3.m1" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math> strikes an effective trade-off between accuracy and computational efficiency.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p">The essential purpose of representing tokens as vectors is to use the cosine similarity between these high dimension vectors to represent semantic differences. The simple algebraic operations on vectors may not always work, because they do not necessarily reflect semantic relationships. For example, the conceptual illustration in the following may work for some tokens, but not apply to every token <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib23" title="">23</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{s}(\text{King})-\mathbf{s}(\text{Men})+\mathbf{s}(\text{Woman})\approx\mathbf{s}(\text{Queen})." class="ltx_Math" display="block" id="S4.E33.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mtext>King</mtext><mo stretchy="false">)</mo></mrow></mrow><mo>‚àí</mo><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mtext>Men</mtext><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mtext>Woman</mtext><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‚âà</mo><mrow><mi>ùê¨</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mtext>Queen</mtext><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{s}(\text{King})-\mathbf{s}(\text{Men})+\mathbf{s}(\text{Woman})\approx\mathbf{s}(\text{Queen}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, this example effectively demonstrates a projection do exist between the vector representations of ‚ÄúKing‚Äù and ‚ÄúMen‚Äù. Consequently, scalars alone are insufficient to fully characterize the semantic relations. Moreover, the cosine similarity is invariant to rotation and scaling, and much more robust than Euclidean distance in high dimension space. Thus, the cosine similarity and probability measure in <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="S4.SS2.p4.m1" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> are of fundamental importance. Following the idea of Gromov-Wasserstein distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib47" title="">47</a>]</cite>, we define the distance of two semantic vector spaces as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn12">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 12</span></span></h6>
<div class="ltx_para" id="Thmdefn12.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="Thmdefn12.p1.m1" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> and <math alttext="\mathsf{S}^{\prime}" class="ltx_Math" display="inline" id="Thmdefn12.p1.m2" intent=":literal"><semantics><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><annotation encoding="application/x-tex">\mathsf{S}^{\prime}</annotation></semantics></math> be two semantic vector spaces with probability measures <math alttext="\mu" class="ltx_Math" display="inline" id="Thmdefn12.p1.m3" intent=":literal"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> and <math alttext="\nu" class="ltx_Math" display="inline" id="Thmdefn12.p1.m4" intent=":literal"><semantics><mi>ŒΩ</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>, respectively. The squared distance between <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="Thmdefn12.p1.m5" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> and <math alttext="\mathsf{S}^{\prime}" class="ltx_Math" display="inline" id="Thmdefn12.p1.m6" intent=":literal"><semantics><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><annotation encoding="application/x-tex">\mathsf{S}^{\prime}</annotation></semantics></math> is defined as:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})=\min_{\pi\in\Pi(\mu,\nu)}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\left|\mathbf{s}_{1}^{T}\mathbf{s}^{\prime}_{1}-\mathbf{s}_{2}^{T}\mathbf{s}^{\prime}_{2}\right|^{2}d\pi(\mathbf{s}_{1},\mathbf{s}_{2})d\pi(\mathbf{s}^{\prime}_{1},\mathbf{s}^{\prime}_{2})," class="ltx_Math" display="block" id="S4.E34.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>d</mi><mi>s</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùñ≤</mi><mo>,</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>œÄ</mi><mo>‚àà</mo><mrow><mi mathvariant="normal">Œ†</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œº</mi><mo>,</mo><mi>ŒΩ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><msub><mo rspace="0em">‚à´</mo><mrow><mi>ùñ≤</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup></mrow></msub><mrow><msub><mo rspace="0em">‚à´</mo><mrow><mi>ùñ≤</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup></mrow></msub><mrow><msup><mrow><mo>|</mo><mrow><mrow><msubsup><mi>ùê¨</mi><mn>1</mn><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mi>ùê¨</mi><mn>1</mn><mo>‚Ä≤</mo></msubsup></mrow><mo>‚àí</mo><mrow><msubsup><mi>ùê¨</mi><mn>2</mn><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mi>ùê¨</mi><mn>2</mn><mo>‚Ä≤</mo></msubsup></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo rspace="0em">ùëë</mo><mi>œÄ</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><msub><mi>ùê¨</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo rspace="0em">ùëë</mo><mi>œÄ</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>ùê¨</mi><mn>1</mn><mo>‚Ä≤</mo></msubsup><mo>,</mo><msubsup><mi>ùê¨</mi><mn>2</mn><mo>‚Ä≤</mo></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})=\min_{\pi\in\Pi(\mu,\nu)}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\left|\mathbf{s}_{1}^{T}\mathbf{s}^{\prime}_{1}-\mathbf{s}_{2}^{T}\mathbf{s}^{\prime}_{2}\right|^{2}d\pi(\mathbf{s}_{1},\mathbf{s}_{2})d\pi(\mathbf{s}^{\prime}_{1},\mathbf{s}^{\prime}_{2}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(34)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\Pi(\mu,\nu)" class="ltx_Math" display="inline" id="Thmdefn12.p1.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">Œ†</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œº</mi><mo>,</mo><mi>ŒΩ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Pi(\mu,\nu)</annotation></semantics></math> is the set of all transportation plans between <math alttext="\mu" class="ltx_Math" display="inline" id="Thmdefn12.p1.m8" intent=":literal"><semantics><mi>Œº</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> and <math alttext="\nu" class="ltx_Math" display="inline" id="Thmdefn12.p1.m9" intent=":literal"><semantics><mi>ŒΩ</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p">The definition seeks to find an optimal transport plan <math alttext="\pi" class="ltx_Math" display="inline" id="S4.SS2.p5.m1" intent=":literal"><semantics><mi>œÄ</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> that minimizes the weighted average of the ‚Äúinternal cosine similarity difference‚Äù for all pairs of points, measured before and after the transport. The distance difference imposes a high cost on pairings that distort the intrinsic geometry of two semantic vector spaces. Therefore, if <math alttext="d_{s}(\mathsf{S},\mathsf{S}^{\prime})=0" class="ltx_Math" display="inline" id="S4.SS2.p5.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùñ≤</mi><mo>,</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d_{s}(\mathsf{S},\mathsf{S}^{\prime})=0</annotation></semantics></math>, <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="S4.SS2.p5.m3" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> and <math alttext="\mathsf{S}^{\prime}" class="ltx_Math" display="inline" id="S4.SS2.p5.m4" intent=":literal"><semantics><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><annotation encoding="application/x-tex">\mathsf{S}^{\prime}</annotation></semantics></math> are equivalent in the sense of token-level semantics, which results in an easy translation between these two languages. In fact, the Gromov-Wasserstein distance has already been successfully applied to the alignment of two word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib81" title="">81</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_rmk" id="Thmrmk3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 3</span></span><span class="ltx_text ltx_font_bold"> (Vectorization in Information Theory)</span>
</h6>
<div class="ltx_para" id="Thmrmk3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The relationship between semantic space and semantic vector space is similar to the relationship between information theory and signal processing. Information theory, based on probability theory, is a framework for understanding the nature and limits of information compression, transmission, and storage. However, it is not particularly concerned with the specific methods of implementation in practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib16" title="">16</a>]</cite>. Signal processing, on the other hand, represents information as vectors in <math alttext="\mathbb{R}^{n}" class="ltx_Math" display="inline" id="Thmrmk3.p1.m1" intent=":literal"><semantics><msup><mi>‚Ñù</mi><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{n}</annotation></semantics></math> or <math alttext="\mathbb{C}^{n}" class="ltx_Math" display="inline" id="Thmrmk3.p1.m2" intent=":literal"><semantics><msup><mi>‚ÑÇ</mi><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^{n}</annotation></semantics></math>, making it suitable for sensing, transmission, and storage in physical media. This representation enables a vast body of mathematical theory to be applied to the design of efficient algorithms for practical sensing, communication, and storage systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib82" title="">82</a>]</cite>.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-C </span><span class="ltx_text ltx_font_italic">Semantic Compression/De-dimensionality</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">In information theory, the objective of source coding is to use as few bits as possible to represent a source symbol, such that the source message can be exactly recovered for lossless compression or recovered within a given distortion for lossy compression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib43" title="">43</a>]</cite>. According to Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn11" title="Definition 11 ‚Ä£ IV-B Token-level Semantic Vector Space ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">11</span></a>, however, <math alttext="|\Omega|=N" class="ltx_Math" display="inline" id="S4.SS3.p1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">|</mo></mrow><mo>=</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">|\Omega|=N</annotation></semantics></math> implies <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS3.p1.m2" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math> is a very high dimension sphere such that the direct computation on <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="S4.SS3.p1.m3" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> is still not practical. Extensive experimental results suggest that the choice of dimensionality for a semantic vector space involves a crucial trade-off, implying the existence of an optimal range or ‚Äúsweet spot‚Äù <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib83" title="">83</a>]</cite>. In this case, the semantic compression is the compression of the entire semantic space, i.e., dimension reduction that preserves cosine similarity.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p">In practice, the random projection is widely used to reduce the dimensionally of vectors. The distance conservation property is guaranteed by Johnson-Lindenstrauss (JL) lemma <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib84" title="">84</a>]</cite>. In the following, we introduce the cosine similarity based JL lemma without proof <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib85" title="">85</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_lem" id="Thmlem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 1</span></span></h6>
<div class="ltx_para" id="Thmlem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\epsilon\in(0,1)" class="ltx_Math" display="inline" id="Thmlem1.p1.m1" intent=":literal"><semantics><mrow><mi>œµ</mi><mo>‚àà</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\epsilon\in(0,1)</annotation></semantics></math> and <math alttext="\{\mathbf{s}_{1},\ldots,\mathbf{s}_{M}\}\in\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="Thmlem1.p1.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">{</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>ùê¨</mi><mi>M</mi></msub><mo stretchy="false">}</mo></mrow><mo>‚àà</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\{\mathbf{s}_{1},\ldots,\mathbf{s}_{M}\}\in\mathbb{S}^{N-1}</annotation></semantics></math>, if <math alttext="m\geq\frac{C}{\epsilon^{2}}\log M" class="ltx_Math" display="inline" id="Thmlem1.p1.m3" intent=":literal"><semantics><mrow><mi>m</mi><mo>‚â•</mo><mrow><mfrac><mi>C</mi><msup><mi>œµ</mi><mn>2</mn></msup></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>M</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">m\geq\frac{C}{\epsilon^{2}}\log M</annotation></semantics></math>, there exists a matrix <math alttext="\mathbf{A}\in\mathbb{R}^{m\times N}" class="ltx_Math" display="inline" id="Thmlem1.p1.m4" intent=":literal"><semantics><mrow><mi>ùêÄ</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{A}\in\mathbb{R}^{m\times N}</annotation></semantics></math> such that:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E35">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="|\mathbf{s}_{i}^{T}\mathbf{s}_{j}-\mathbf{s}_{i}^{T}\mathbf{P}\mathbf{s}_{j}|\leq\epsilon,\quad\forall i,j\in\{1,\ldots,M\}," class="ltx_Math" display="block" id="S4.E35.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mo stretchy="false">|</mo><mrow><mrow><msubsup><mi>ùê¨</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mi>j</mi></msub></mrow><mo>‚àí</mo><mrow><msubsup><mi>ùê¨</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêèùê¨</mi><mi>j</mi></msub></mrow></mrow><mo stretchy="false">|</mo></mrow><mo>‚â§</mo><mrow><mi>œµ</mi><mo rspace="1.167em">,</mo><mrow><mo rspace="0.167em">‚àÄ</mo><mi>i</mi></mrow></mrow></mrow><mo>,</mo><mrow><mi>j</mi><mo>‚àà</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>M</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">|\mathbf{s}_{i}^{T}\mathbf{s}_{j}-\mathbf{s}_{i}^{T}\mathbf{P}\mathbf{s}_{j}|\leq\epsilon,\quad\forall i,j\in\{1,\ldots,M\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(35)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\mathbf{P}=\mathbf{A}^{T}\mathbf{A}" class="ltx_Math" display="inline" id="Thmlem1.p1.m5" intent=":literal"><semantics><mrow><mi>ùêè</mi><mo>=</mo><mrow><msup><mi>ùêÄ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùêÄ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{P}=\mathbf{A}^{T}\mathbf{A}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p">According to JL lemma, the dimensionality of the semantic vector space can be reduced from <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.p3.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> to <math alttext="m\geq\frac{C}{\epsilon^{2}}\log M" class="ltx_Math" display="inline" id="S4.SS3.p3.m2" intent=":literal"><semantics><mrow><mi>m</mi><mo>‚â•</mo><mrow><mfrac><mi>C</mi><msup><mi>œµ</mi><mn>2</mn></msup></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>M</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">m\geq\frac{C}{\epsilon^{2}}\log M</annotation></semantics></math>. As aforementioned, each semantic vector can be seen as a real signal vector which should be very sparse in <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS3.p3.m3" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math>. Inspired by compressive sensing, the cosine similarity based JL lemma can be improved by applying restricted isometry property (RIP). Let <math alttext="\mathbf{A}" class="ltx_Math" display="inline" id="S4.SS3.p3.m4" intent=":literal"><semantics><mi>ùêÄ</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math> be a matrix satisfying <math alttext="(k,\delta)" class="ltx_Math" display="inline" id="S4.SS3.p3.m5" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>,</mo><mi>Œ¥</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k,\delta)</annotation></semantics></math>-RIP, that is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E36">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="1-\delta\leq\|\mathbf{As}\|^{2}\leq 1+\delta," class="ltx_Math" display="block" id="S4.E36.m1" intent=":literal"><semantics><mrow><mrow><mrow><mn>1</mn><mo>‚àí</mo><mi>Œ¥</mi></mrow><mo>‚â§</mo><msup><mrow><mo stretchy="false">‚Äñ</mo><mi>ùêÄùê¨</mi><mo stretchy="false">‚Äñ</mo></mrow><mn>2</mn></msup><mo>‚â§</mo><mrow><mn>1</mn><mo>+</mo><mi>Œ¥</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">1-\delta\leq\|\mathbf{As}\|^{2}\leq 1+\delta,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(36)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for all <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p3.m6" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-sparse <math alttext="\mathbf{s}\in\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS3.p3.m7" intent=":literal"><semantics><mrow><mi>ùê¨</mi><mo>‚àà</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{s}\in\mathbb{S}^{N-1}</annotation></semantics></math>, i.e., <math alttext="\|\mathbf{s}\|_{0}\leq k" class="ltx_Math" display="inline" id="S4.SS3.p3.m8" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‚Äñ</mo><mi>ùê¨</mi><mo stretchy="false">‚Äñ</mo></mrow><mn>0</mn></msub><mo>‚â§</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">\|\mathbf{s}\|_{0}\leq k</annotation></semantics></math>. The following result is established in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib86" title="">86</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm3">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3</span></span></h6>
<div class="ltx_para" id="Thmthm3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\eta,\epsilon\in(0,1)" class="ltx_Math" display="inline" id="Thmthm3.p1.m1" intent=":literal"><semantics><mrow><mrow><mi>Œ∑</mi><mo>,</mo><mi>œµ</mi></mrow><mo>‚àà</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\eta,\epsilon\in(0,1)</annotation></semantics></math>, <math alttext="\{\mathbf{s}_{1},\ldots,\mathbf{s}_{M}\}\in\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="Thmthm3.p1.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">{</mo><msub><mi>ùê¨</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>ùê¨</mi><mi>M</mi></msub><mo stretchy="false">}</mo></mrow><mo>‚àà</mo><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\{\mathbf{s}_{1},\ldots,\mathbf{s}_{M}\}\in\mathbb{S}^{N-1}</annotation></semantics></math>, and <math alttext="\mathbf{A}\in\mathbb{R}^{m\times N}" class="ltx_Math" display="inline" id="Thmthm3.p1.m3" intent=":literal"><semantics><mrow><mi>ùêÄ</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{A}\in\mathbb{R}^{m\times N}</annotation></semantics></math> be <math alttext="(k,\delta)" class="ltx_Math" display="inline" id="Thmthm3.p1.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>,</mo><mi>Œ¥</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k,\delta)</annotation></semantics></math>-RIP with <math alttext="\delta\leq\epsilon/4" class="ltx_Math" display="inline" id="Thmthm3.p1.m5" intent=":literal"><semantics><mrow><mi>Œ¥</mi><mo>‚â§</mo><mrow><mi>œµ</mi><mo>/</mo><mn>4</mn></mrow></mrow><annotation encoding="application/x-tex">\delta\leq\epsilon/4</annotation></semantics></math> and <math alttext="k\geq 40\log\frac{4M}{\eta}" class="ltx_Math" display="inline" id="Thmthm3.p1.m6" intent=":literal"><semantics><mrow><mi>k</mi><mo>‚â•</mo><mrow><mn>40</mn><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mrow><mn>4</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mi>M</mi></mrow><mi>Œ∑</mi></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">k\geq 40\log\frac{4M}{\eta}</annotation></semantics></math>. Let <math alttext="\bm{\sigma}" class="ltx_Math" display="inline" id="Thmthm3.p1.m7" intent=":literal"><semantics><mi>ùõî</mi><annotation encoding="application/x-tex">\bm{\sigma}</annotation></semantics></math> a Rademacher sequence, i.e., uniformly distributed on <math alttext="\{-1,1\}^{N}" class="ltx_Math" display="inline" id="Thmthm3.p1.m8" intent=":literal"><semantics><msup><mrow><mo stretchy="false">{</mo><mrow><mo>‚àí</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><mi>N</mi></msup><annotation encoding="application/x-tex">\{-1,1\}^{N}</annotation></semantics></math>. Then, with probability exceeding <math alttext="1-\eta" class="ltx_Math" display="inline" id="Thmthm3.p1.m9" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚àí</mo><mi>Œ∑</mi></mrow><annotation encoding="application/x-tex">1-\eta</annotation></semantics></math>,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E37">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="|\mathbf{s}_{i}^{T}\mathbf{s}_{j}-\mathbf{s}_{i}^{T}\mathbf{D}_{\bm{\sigma}}\mathbf{P}\mathbf{D}_{\bm{\sigma}}\mathbf{s}_{j}|\leq\epsilon,\quad\forall i,j\in\{1,\ldots,M\}," class="ltx_Math" display="block" id="S4.E37.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mo stretchy="false">|</mo><mrow><mrow><msubsup><mi>ùê¨</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mi>j</mi></msub></mrow><mo>‚àí</mo><mrow><msubsup><mi>ùê¨</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÉ</mi><mi>ùùà</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêèùêÉ</mi><mi>ùùà</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mi>j</mi></msub></mrow></mrow><mo stretchy="false">|</mo></mrow><mo>‚â§</mo><mrow><mi>œµ</mi><mo rspace="1.167em">,</mo><mrow><mo rspace="0.167em">‚àÄ</mo><mi>i</mi></mrow></mrow></mrow><mo>,</mo><mrow><mi>j</mi><mo>‚àà</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>M</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">|\mathbf{s}_{i}^{T}\mathbf{s}_{j}-\mathbf{s}_{i}^{T}\mathbf{D}_{\bm{\sigma}}\mathbf{P}\mathbf{D}_{\bm{\sigma}}\mathbf{s}_{j}|\leq\epsilon,\quad\forall i,j\in\{1,\ldots,M\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(37)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\mathbf{D}_{\bm{\sigma}}" class="ltx_Math" display="inline" id="Thmthm3.p1.m10" intent=":literal"><semantics><msub><mi>ùêÉ</mi><mi>ùõî</mi></msub><annotation encoding="application/x-tex">\mathbf{D}_{\bm{\sigma}}</annotation></semantics></math> is a diagonal matrix whose diagonal entries are the elements of the vector <math alttext="\bm{\sigma}" class="ltx_Math" display="inline" id="Thmthm3.p1.m11" intent=":literal"><semantics><mi>ùõî</mi><annotation encoding="application/x-tex">\bm{\sigma}</annotation></semantics></math> and <math alttext="\mathbf{P}=\mathbf{A}^{T}\mathbf{A}" class="ltx_Math" display="inline" id="Thmthm3.p1.m12" intent=":literal"><semantics><mrow><mi>ùêè</mi><mo>=</mo><mrow><msup><mi>ùêÄ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùêÄ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{P}=\mathbf{A}^{T}\mathbf{A}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p">According to the theory of compressive sensing, the <math alttext="m\times N" class="ltx_Math" display="inline" id="S4.SS3.p4.m1" intent=":literal"><semantics><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">m\times N</annotation></semantics></math> partial Gaussian matrix can be used with</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E38">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m\geq\frac{C}{\epsilon^{2}}\log\frac{M}{\eta}\log N," class="ltx_Math" display="block" id="S4.E38.m1" intent=":literal"><semantics><mrow><mrow><mi>m</mi><mo>‚â•</mo><mrow><mfrac><mi>C</mi><msup><mi>œµ</mi><mn>2</mn></msup></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mrow><mfrac><mi>M</mi><mi>Œ∑</mi></mfrac><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>N</mi></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">m\geq\frac{C}{\epsilon^{2}}\log\frac{M}{\eta}\log N,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(38)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">but the complexity of the matrix-vector multiplication is very high. However, <math alttext="\mathbf{A}" class="ltx_Math" display="inline" id="S4.SS3.p4.m2" intent=":literal"><semantics><mi>ùêÄ</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math> can also be obtained by randomly selecting <math alttext="m" class="ltx_Math" display="inline" id="S4.SS3.p4.m3" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> rows from the discrete Fourier transform (DFT) matrix, discrete cosine transform (DCT) matrix, or Hadamard matrix. In this case, <math alttext="m" class="ltx_Math" display="inline" id="S4.SS3.p4.m4" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> will be larger than using partial Gaussian matrix, but the complexity is greatly reduced.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p">Recalling Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn12" title="Definition 12 ‚Ä£ IV-B Token-level Semantic Vector Space ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">12</span></a>, the distortion of semantic compression can be evaluated by the distance of two semantic vector spaces. Let <math alttext="\mathsf{S}" class="ltx_Math" display="inline" id="S4.SS3.p5.m1" intent=":literal"><semantics><mi>ùñ≤</mi><annotation encoding="application/x-tex">\mathsf{S}</annotation></semantics></math> be the original semantic vector space on <math alttext="\mathbb{S}^{N-1}" class="ltx_Math" display="inline" id="S4.SS3.p5.m2" intent=":literal"><semantics><msup><mi>ùïä</mi><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{N-1}</annotation></semantics></math> and <math alttext="\mathsf{S}^{\prime}" class="ltx_Math" display="inline" id="S4.SS3.p5.m3" intent=":literal"><semantics><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><annotation encoding="application/x-tex">\mathsf{S}^{\prime}</annotation></semantics></math> on <math alttext="\mathbb{S}^{m}" class="ltx_Math" display="inline" id="S4.SS3.p5.m4" intent=":literal"><semantics><msup><mi>ùïä</mi><mi>m</mi></msup><annotation encoding="application/x-tex">\mathbb{S}^{m}</annotation></semantics></math> with <math alttext="1\leq m&lt;N-1" class="ltx_Math" display="inline" id="S4.SS3.p5.m5" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚â§</mo><mi>m</mi><mo>&lt;</mo><mrow><mi>N</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">1\leq m&lt;N-1</annotation></semantics></math>, the distortion of semantic compression can be written as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E39">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})=\min_{\pi\in\Pi(\mu,\mu^{\prime})}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\left|\mathbf{s}^{T}\mathbf{s}^{\prime}-\mathbf{s}^{T}\mathbf{P}\mathbf{s}^{\prime}\right|^{2}d\pi(\mathbf{s},\mathbf{As})d\pi(\mathbf{s}^{\prime},\mathbf{As}^{\prime})," class="ltx_Math" display="block" id="S4.E39.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>d</mi><mi>s</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùñ≤</mi><mo>,</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>œÄ</mi><mo>‚àà</mo><mrow><mi mathvariant="normal">Œ†</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œº</mi><mo>,</mo><msup><mi>Œº</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><msub><mo rspace="0em">‚à´</mo><mrow><mi>ùñ≤</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup></mrow></msub><mrow><msub><mo rspace="0em">‚à´</mo><mrow><mi>ùñ≤</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup></mrow></msub><mrow><msup><mrow><mo>|</mo><mrow><mrow><msup><mi>ùê¨</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><msup><mi>ùê¨</mi><mo>‚Ä≤</mo></msup></mrow><mo>‚àí</mo><mrow><msup><mi>ùê¨</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><msup><mi>ùêèùê¨</mi><mo>‚Ä≤</mo></msup></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo rspace="0em">ùëë</mo><mi>œÄ</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê¨</mi><mo>,</mo><mi>ùêÄùê¨</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo rspace="0em">ùëë</mo><mi>œÄ</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msup><mi>ùê¨</mi><mo>‚Ä≤</mo></msup><mo>,</mo><msup><mi>ùêÄùê¨</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})=\min_{\pi\in\Pi(\mu,\mu^{\prime})}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\int_{\mathsf{S}\times\mathsf{S}^{\prime}}\left|\mathbf{s}^{T}\mathbf{s}^{\prime}-\mathbf{s}^{T}\mathbf{P}\mathbf{s}^{\prime}\right|^{2}d\pi(\mathbf{s},\mathbf{As})d\pi(\mathbf{s}^{\prime},\mathbf{As}^{\prime}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(39)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{A}" class="ltx_Math" display="inline" id="S4.SS3.p5.m6" intent=":literal"><semantics><mi>ùêÄ</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math> is a <math alttext="m\times N" class="ltx_Math" display="inline" id="S4.SS3.p5.m7" intent=":literal"><semantics><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">m\times N</annotation></semantics></math> projection matrix and <math alttext="\mathbf{P}=\mathbf{A}^{T}\mathbf{A}" class="ltx_Math" display="inline" id="S4.SS3.p5.m8" intent=":literal"><semantics><mrow><mi>ùêè</mi><mo>=</mo><mrow><msup><mi>ùêÄ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùêÄ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{P}=\mathbf{A}^{T}\mathbf{A}</annotation></semantics></math>. The following theorem can be established by applying Lemma <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmlem1" title="Lemma 1 ‚Ä£ IV-C Semantic Compression/De-dimensionality ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> or Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm3" title="Theorem 3 ‚Ä£ IV-C Semantic Compression/De-dimensionality ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">3</span></a> directly.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm4">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4</span></span></h6>
<div class="ltx_para" id="Thmthm4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The distortion of semantic compression can be bounded by <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmthm4.p1.m1" intent=":literal"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>, i.e., <math alttext="d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})\leq\epsilon" class="ltx_Math" display="inline" id="Thmthm4.p1.m2" intent=":literal"><semantics><mrow><mrow><msubsup><mi>d</mi><mi>s</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùñ≤</mi><mo>,</mo><msup><mi>ùñ≤</mi><mo>‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>‚â§</mo><mi>œµ</mi></mrow><annotation encoding="application/x-tex">d_{s}^{2}(\mathsf{S},\mathsf{S}^{\prime})\leq\epsilon</annotation></semantics></math>, with high probability.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p">The semantic compression/de-dimensionality discussed in this subsection does not consider the distribution on semantic vector space. Therefore, the bound in Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm4" title="Theorem 4 ‚Ä£ IV-C Semantic Compression/De-dimensionality ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> is not tight, yet far from optimal in the sense of information theory. Similar to rate-distortion theory, the dimension-distortion theory can be further developed for semantic compression, especially for the case of <math alttext="m" class="ltx_Math" display="inline" id="S4.SS3.p6.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> smaller than the threshold in Lemma <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmlem1" title="Lemma 1 ‚Ä£ IV-C Semantic Compression/De-dimensionality ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> or Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm3" title="Theorem 3 ‚Ä£ IV-C Semantic Compression/De-dimensionality ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_theorem ltx_theorem_rmk" id="Thmrmk4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 4</span></span><span class="ltx_text ltx_font_bold"> (Approximate Nearest Neighbor Search)</span>
</h6>
<div class="ltx_para" id="Thmrmk4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Vector databases are regarded as a critical piece of infrastructure for helping LLMs mitigate hallucinations. They can also store vast amounts of private and proprietary data, enhancing the capabilities of LLMs in vertical domains. Consequently, approximate nearest neighbor (ANN) vector search algorithms stand out as a key technology that integrates vector databases with LLMs. From the perspective of information theory, the nearest ANN vector searching is an extension to decoding algorithm, which is to search the nearest codeword for the received symbols. Since 2023, the ANN vector search algorithms proposed by the experts from our lab have been ranked TOP-1 on ANN-Benchmarks leader-board.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">2</span></span><span class="ltx_text ltx_font_upright">https://ann-benchmarks.com.</span></span></span></span> Interested researchers can access our code repository.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">3</span></span><span class="ltx_text ltx_font_upright">https://github.com/WPJiang/HWTL_SDU-ANNS.</span></span></span></span></span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-D </span><span class="ltx_text ltx_font_italic">Semantic Embedding/Vectorization for Next-token Prediction</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p">In practice, we typically select a proper dimension <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.p1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> to directly perform the semantic embedding or vectorization. In the following, we will discuss information-theoretically optimal approach. It is natural to understand that the semantics of an utterance highly depend on the speaker‚Äôs intended goal, i.e., the downstream task in machine learning. Therefore, for a token sequence with length <math alttext="n" class="ltx_Math" display="inline" id="S4.SS4.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, the semantic embedding is a mapping <math alttext="f:\Omega^{n}\to(\mathbb{S}^{m})^{n}" class="ltx_Math" display="inline" id="S4.SS4.p1.m3" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi mathvariant="normal">Œ©</mi><mi>n</mi></msup><mo stretchy="false">‚Üí</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ùïä</mi><mi>m</mi></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f:\Omega^{n}\to(\mathbb{S}^{m})^{n}</annotation></semantics></math>, such that a loss functional <math alttext="L(f)" class="ltx_Math" display="inline" id="S4.SS4.p1.m4" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(f)</annotation></semantics></math>, defined by the downstream task, is minimized.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p">From the perspective of LLMs, the objective is to predict the next token based on the prompt and the parameterized memory. Therefore, <math alttext="L(f)" class="ltx_Math" display="inline" id="S4.SS4.p2.m1" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(f)</annotation></semantics></math> should be designed to best facilitate of achieving this goal. Let <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p2.m2" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> be a token sequence, <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p2.m3" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> be the corresponding semantic vector representation of <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p2.m4" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math>. For the task of the next token prediction, <math alttext="S_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.m5" intent=":literal"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation></semantics></math> should contain all the information in <math alttext="X_{1:t}" class="ltx_Math" display="inline" id="S4.SS4.p2.m6" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:t}</annotation></semantics></math> which is useful to predict <math alttext="X_{t+1:n}" class="ltx_Math" display="inline" id="S4.SS4.p2.m7" intent=":literal"><semantics><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{t+1:n}</annotation></semantics></math>. From the perspective of information theory, the optimal semantic encoder for next token prediction should be the solution of the following problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E40">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t}|S_{1:t-1}),\quad 1\leq t\leq n\in\mathbb{N}." class="ltx_Math" display="block" id="S4.E40.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>S</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mn>1</mn></mrow><mo>‚â§</mo><mi>t</mi><mo>‚â§</mo><mi>n</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t}|S_{1:t-1}),\quad 1\leq t\leq n\in\mathbb{N}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(40)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The condition means <math alttext="S_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.m8" intent=":literal"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation></semantics></math> only contains new information for predicting <math alttext="X_{t+1:n}" class="ltx_Math" display="inline" id="S4.SS4.p2.m9" intent=":literal"><semantics><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{t+1:n}</annotation></semantics></math> which is not contained in <math alttext="S_{1:t-1}" class="ltx_Math" display="inline" id="S4.SS4.p2.m10" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><annotation encoding="application/x-tex">S_{1:t-1}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p">The solution of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.E40" title="In IV-D Semantic Embedding/Vectorization for Next-token Prediction ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">40</span></a>) maximizes the backward directed information <math alttext="I(X_{n:1}\to S_{1:n})" class="ltx_Math" display="inline" id="S4.SS4.p3.m1" intent=":literal"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(X_{n:1}\to S_{1:n})</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E41">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I^{*}(X_{n:1}\to S_{1:n})=\sum_{t=1}^{n}\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t}|S_{1:t-1})." class="ltx_Math" display="block" id="S4.E41.m1" intent=":literal"><semantics><mrow><mrow><mrow><msup><mi>I</mi><mo>‚àó</mo></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><msub><mi>S</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I^{*}(X_{n:1}\to S_{1:n})=\sum_{t=1}^{n}\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t}|S_{1:t-1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(41)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Following the inequalities of directed information in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib64" title="">64</a>]</cite>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E42">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I^{*}(X_{n:1}\to S_{1:n})\leq\sum_{t=1}^{n}\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t})\leq\sum_{t=1}^{n}\sum_{k=1}^{n-t}\max_{S_{t}=f(X_{1:t})}I(X_{t+k};S_{t})." class="ltx_Math" display="block" id="S4.E42.m1" intent=":literal"><semantics><mrow><mrow><mrow><msup><mi>I</mi><mo>‚àó</mo></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">‚â§</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.111em">‚â§</mo><mrow><munderover><mo movablelimits="false" rspace="0em">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>‚àí</mo><mi>t</mi></mrow></munderover><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub><mo>;</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I^{*}(X_{n:1}\to S_{1:n})\leq\sum_{t=1}^{n}\max_{S_{t}=f(X_{1:t})}I(X_{t+1:n};S_{t})\leq\sum_{t=1}^{n}\sum_{k=1}^{n-t}\max_{S_{t}=f(X_{1:t})}I(X_{t+k};S_{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(42)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p">Inspired by the idea of predictive coding in information theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib88" title="">88</a>]</cite>, the CPC is proposed for semantic embedding in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib48" title="">48</a>]</cite>, which is also adopt in OpenAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib49" title="">49</a>]</cite>. Let <math alttext="Z_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p4.m1" intent=":literal"><semantics><msub><mi>Z</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">Z_{1:n}</annotation></semantics></math> be the latent representation of <math alttext="X_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p4.m2" intent=":literal"><semantics><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">X_{1:n}</annotation></semantics></math> with <math alttext="Z_{t}=g_{\text{ENC}}(X_{t})" class="ltx_Math" display="inline" id="S4.SS4.p4.m3" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>g</mi><mtext>ENC</mtext></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Z_{t}=g_{\text{ENC}}(X_{t})</annotation></semantics></math>, <math alttext="S_{1:n}" class="ltx_Math" display="inline" id="S4.SS4.p4.m4" intent=":literal"><semantics><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">S_{1:n}</annotation></semantics></math> be the semantic vector obtained by CPC, which is defined as <math alttext="S_{t}=g_{\text{AR}}(Z_{1:t-1})" class="ltx_Math" display="inline" id="S4.SS4.p4.m5" intent=":literal"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>g</mi><mtext>AR</mtext></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">S_{t}=g_{\text{AR}}(Z_{1:t-1})</annotation></semantics></math>. The training process of CPC is to solve the following optimization problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E43">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{k=1}^{n-t}\max_{S_{t}=f(X_{1:t})}I(X_{t+k};S_{t})." class="ltx_Math" display="block" id="S4.E43.m1" intent=":literal"><semantics><mrow><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>‚àí</mo><mi>t</mi></mrow></munderover><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub><mo>;</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{n-t}\max_{S_{t}=f(X_{1:t})}I(X_{t+k};S_{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(43)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, the CPC maximizes the upper-bound of <math alttext="I^{*}(X_{n:1}\to S_{1:n})" class="ltx_Math" display="inline" id="S4.SS4.p4.m6" intent=":literal"><semantics><mrow><msup><mi>I</mi><mo>‚àó</mo></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mn>1</mn></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I^{*}(X_{n:1}\to S_{1:n})</annotation></semantics></math>, which is a sub-optimal semantic encoder from the perspective of information theory. In this context, the information theoretical optimal semantic embedding can be achieved, if we can optimize the backward directed information Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4.E41" title="In IV-D Semantic Embedding/Vectorization for Next-token Prediction ‚Ä£ IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">41</span></a>) or its tighter upper bound.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps">Autoregression LLMs</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">In this section, we focus on LLMs with a special architecture, i.e., AR-LLMs. The Transformer architecture and its performance can be derived from our general definition. Other LLM architectures, such as Mamba/Mamba2 and LLaDA, are also discussed.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-A </span><span class="ltx_text ltx_font_italic">TV-VAR based AR-LLMs</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">Let <math alttext="\mathbf{s}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p1.m1" intent=":literal"><semantics><msub><mi>ùê¨</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{s}_{t}</annotation></semantics></math> with <math alttext="t=1,\ldots,n" class="ltx_Math" display="inline" id="S5.SS1.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\ldots,n</annotation></semantics></math> and <math alttext="\mathbf{u}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p1.m3" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{t}</annotation></semantics></math> with <math alttext="t=n+1,\ldots,T" class="ltx_Math" display="inline" id="S5.SS1.p1.m4" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=n+1,\ldots,T</annotation></semantics></math> be sample vectors of random variables <math alttext="S_{t}" class="ltx_Math" display="inline" id="S5.SS1.p1.m5" intent=":literal"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation></semantics></math> and <math alttext="U_{t}" class="ltx_Math" display="inline" id="S5.SS1.p1.m6" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math>.
To simplify the notation, we let <math alttext="\mathbf{u}_{t}=\mathbf{s}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p1.m7" intent=":literal"><semantics><mrow><msub><mi>ùêÆ</mi><mi>t</mi></msub><mo>=</mo><msub><mi>ùê¨</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{u}_{t}=\mathbf{s}_{t}</annotation></semantics></math> for <math alttext="t=1,\ldots,n" class="ltx_Math" display="inline" id="S5.SS1.p1.m8" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\ldots,n</annotation></semantics></math>. We then have the following definition.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn13">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 13</span></span></h6>
<div class="ltx_para" id="Thmdefn13.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The TV-VAR based AR-LLM is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E44">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\mathbf{A}_{tj}\mathbf{u}_{j}\right)\right),\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S5.E44.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ùêÆ</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">‚Å°</mo><mi>softmax</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi mathvariant="normal">Œû</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÆ</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\mathbf{A}_{tj}\mathbf{u}_{j}\right)\right),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(44)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\mathbf{A}_{tj}" class="ltx_Math" display="inline" id="Thmdefn13.p1.m1" intent=":literal"><semantics><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{A}_{tj}</annotation></semantics></math> is the coefficient matrix, <math alttext="\tilde{\mathbf{u}}_{1:N}" class="ltx_Math" display="inline" id="Thmdefn13.p1.m2" intent=":literal"><semantics><msub><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow></msub><annotation encoding="application/x-tex">\tilde{\mathbf{u}}_{1:N}</annotation></semantics></math> are all possible token vectors in <math alttext="\mathcal{S}(\Omega)" class="ltx_Math" display="inline" id="Thmdefn13.p1.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}(\Omega)</annotation></semantics></math>, and <math alttext="\Xi" class="ltx_Math" display="inline" id="Thmdefn13.p1.m4" intent=":literal"><semantics><mi mathvariant="normal">Œû</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math> is the sampling temperature.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p">In contrast to the standard VAR model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib50" title="">50</a>]</cite>, <math alttext="\mathbf{A}_{tj}" class="ltx_Math" display="inline" id="S5.SS1.p2.m1" intent=":literal"><semantics><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{A}_{tj}</annotation></semantics></math> is time-variant, which is very difficult to estimate in practice.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-B </span><span class="ltx_text ltx_font_italic">Transformer Architecture</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">Consider a decomposition of <math alttext="\mathbf{A}_{tj}" class="ltx_Math" display="inline" id="S5.SS2.p1.m1" intent=":literal"><semantics><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{A}_{tj}</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E45">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{A}_{tj}=\pi_{tj}\mathbf{A}," class="ltx_Math" display="block" id="S5.E45.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùêÄ</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{A}_{tj}=\pi_{tj}\mathbf{A},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(45)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{A}" class="ltx_Math" display="inline" id="S5.SS2.p1.m2" intent=":literal"><semantics><mi>ùêÄ</mi><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math> is a time-invariant parameter matrix, and <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="S5.SS2.p1.m3" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> is the only time-variant scalar weight satisfying <math alttext="\sum_{j=1}^{t-1}\pi_{tj}=1" class="ltx_Math" display="inline" id="S5.SS2.p1.m4" intent=":literal"><semantics><mrow><mrow><msubsup><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msubsup><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{j=1}^{t-1}\pi_{tj}=1</annotation></semantics></math> and <math alttext="\pi_{tj}\geq 0" class="ltx_Math" display="inline" id="S5.SS2.p1.m5" intent=":literal"><semantics><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>‚â•</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\pi_{tj}\geq 0</annotation></semantics></math>. Simple derivation yields the following theorem.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm5">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 5</span></span></h6>
<div class="ltx_para" id="Thmthm5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The Transformer is an AR-LLM with the following form</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E46">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{A}\mathbf{u}_{j}\right)\right),\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S5.E46.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ùêÆ</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">‚Å°</mo><mi>softmax</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi mathvariant="normal">Œû</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÄùêÆ</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{A}\mathbf{u}_{j}\right)\right),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(46)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="Thmthm5.p1.m1" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> is the output of the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="Thmthm5.p1.m2" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math>, that is</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E47">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\pi_{tj}=\frac{\exp(\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{j})}{\sum_{i=1}^{t-1}\exp(\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{i})},\quad j=1,\ldots,t-1." class="ltx_Math" display="block" id="S5.E47.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>ùêÆ</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÅùêÆ</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msubsup><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>ùêÆ</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÅùêÆ</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mrow><mo rspace="1.167em">,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\pi_{tj}=\frac{\exp(\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{j})}{\sum_{i=1}^{t-1}\exp(\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{i})},\quad j=1,\ldots,t-1.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(47)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p">Let <math alttext="\mathbf{q}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m1" intent=":literal"><semantics><msub><mi>ùê™</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{q}_{t}</annotation></semantics></math>, <math alttext="\mathbf{k}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m2" intent=":literal"><semantics><msub><mi>ùê§</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{k}_{t}</annotation></semantics></math>, and <math alttext="\mathbf{v}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m3" intent=":literal"><semantics><msub><mi>ùêØ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{v}_{t}</annotation></semantics></math> be sample vectors of random variables <math alttext="Q_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m4" intent=":literal"><semantics><msub><mi>Q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Q_{t}</annotation></semantics></math>, <math alttext="K_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m5" intent=":literal"><semantics><msub><mi>K</mi><mi>t</mi></msub><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math>, and <math alttext="V_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m6" intent=":literal"><semantics><msub><mi>V</mi><mi>t</mi></msub><annotation encoding="application/x-tex">V_{t}</annotation></semantics></math>. The attention scheme in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib28" title="">28</a>]</cite> implies</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E48">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left\{\begin{aligned} &amp;\mathbf{q}_{t}=\mathbf{W}_{q}\mathbf{u}_{t},\\
&amp;\mathbf{k}_{t}=\mathbf{W}_{k}\mathbf{u}_{t},\\
&amp;\mathbf{v}_{t}=\mathbf{W}_{v}\mathbf{u}_{t},\end{aligned}\right." class="ltx_math_unparsed" display="block" id="S5.E48.m1" intent=":literal"><semantics><mrow><mo>{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><msub><mi>ùê™</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ùêñ</mi><mi>q</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÆ</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><msub><mi>ùê§</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ùêñ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÆ</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><msub><mi>ùêØ</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ùêñ</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÆ</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\left\{\begin{aligned} &amp;\mathbf{q}_{t}=\mathbf{W}_{q}\mathbf{u}_{t},\\
&amp;\mathbf{k}_{t}=\mathbf{W}_{k}\mathbf{u}_{t},\\
&amp;\mathbf{v}_{t}=\mathbf{W}_{v}\mathbf{u}_{t},\end{aligned}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(48)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for <math alttext="t=1,\ldots,T" class="ltx_Math" display="inline" id="S5.SS2.p2.m7" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\ldots,T</annotation></semantics></math>. The output of the Transformer is</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E49">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{v}_{j}\right)\right),\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S5.E49.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ùêÆ</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">‚Å°</mo><mi>softmax</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi mathvariant="normal">Œû</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêØ</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{v}_{j}\right)\right),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(49)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E50">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\pi_{tj}=\frac{\exp(\mathbf{q}_{t-1}^{T}\mathbf{k}_{j})}{\sum_{i=1}^{t-1}\exp(\mathbf{q}_{t-1}^{T}\mathbf{k}_{i})},\quad j=1,\ldots,t-1" class="ltx_Math" display="block" id="S5.E50.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>ùê™</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê§</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msubsup><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>ùê™</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê§</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mrow><mo rspace="1.167em">,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\pi_{tj}=\frac{\exp(\mathbf{q}_{t-1}^{T}\mathbf{k}_{j})}{\sum_{i=1}^{t-1}\exp(\mathbf{q}_{t-1}^{T}\mathbf{k}_{i})},\quad j=1,\ldots,t-1</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(50)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is the attention score. This theorem is established by letting <math alttext="\mathbf{A}=\mathbf{W}_{v}" class="ltx_Math" display="inline" id="S5.SS2.p2.m8" intent=":literal"><semantics><mrow><mi>ùêÄ</mi><mo>=</mo><msub><mi>ùêñ</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{A}=\mathbf{W}_{v}</annotation></semantics></math> and</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E51">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{B}=\mathbf{W}_{q}^{T}\mathbf{W}_{k}." class="ltx_Math" display="block" id="S5.E51.m1" intent=":literal"><semantics><mrow><mrow><mi>ùêÅ</mi><mo>=</mo><mrow><msubsup><mi>ùêñ</mi><mi>q</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêñ</mi><mi>k</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{B}=\mathbf{W}_{q}^{T}\mathbf{W}_{k}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(51)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">‚àé</p>
</div>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p">This theorem shows that the Transformer is equivalent to a decomposition of <math alttext="\mathbf{A}_{tj}" class="ltx_Math" display="inline" id="S5.SS2.p3.m1" intent=":literal"><semantics><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{A}_{tj}</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E52">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{A}_{tj}=\pi_{tj}\mathbf{A}," class="ltx_Math" display="block" id="S5.E52.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ùêÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùêÄ</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{A}_{tj}=\pi_{tj}\mathbf{A},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(52)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="S5.SS2.p3.m2" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> measures the semantic relevance from <math alttext="\mathbf{u}_{j}" class="ltx_Math" display="inline" id="S5.SS2.p3.m3" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{j}</annotation></semantics></math> with <math alttext="j=1,\ldots,t-1" class="ltx_Math" display="inline" id="S5.SS2.p3.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></mrow><annotation encoding="application/x-tex">j=1,\ldots,t-1</annotation></semantics></math> for predicting <math alttext="\mathbf{u}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p3.m5" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{t}</annotation></semantics></math>. In an utterance, the semantic relevance is asymmetric between different tokens. Recalling Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S4" title="IV Vector Representation of Token-level Semantics ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">IV</span></a>, the inner product is used to measure the correlations of token-level semantic. For the asymmetric semantic relevance in an utterance, the inner-product based bilinear form for predicting <math alttext="\mathbf{u}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p3.m6" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{t}</annotation></semantics></math> is introduced as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E53">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="B(\mathbf{u}_{t-1},\mathbf{u}_{j})=\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{j},\quad j=1,\ldots,t-1,\textrm{and }t=n+1,\ldots,T," class="ltx_Math" display="block" id="S5.E53.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>B</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùêÆ</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ùêÆ</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>ùêÆ</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÅùêÆ</mi><mi>j</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mtext>and¬†</mtext><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">B(\mathbf{u}_{t-1},\mathbf{u}_{j})=\mathbf{u}_{t-1}^{T}\mathbf{B}\mathbf{u}_{j},\quad j=1,\ldots,t-1,\textrm{and }t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(53)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{B}\neq\mathbf{B}^{T}" class="ltx_Math" display="inline" id="S5.SS2.p3.m7" intent=":literal"><semantics><mrow><mi>ùêÅ</mi><mo>‚â†</mo><msup><mi>ùêÅ</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{B}\neq\mathbf{B}^{T}</annotation></semantics></math> in general. <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="S5.SS2.p3.m8" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> can then be assigned by using <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS2.p3.m9" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> as Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.E47" title="In Theorem 5 ‚Ä£ V-B Transformer Architecture ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">47</span></a>). According to Jaynes‚Äô maximum entropy principle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib89" title="">89</a>]</cite>, the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS2.p3.m10" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> is a probability assignment on discrete sample space that maximize the entropy with the constraint on the first order moment. Therefore, the obtained estimation of the semantic relevance is the one with the maximum uncertainty, i.e., the best achievable estimation in the worst case.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-C </span><span class="ltx_text ltx_font_italic">ELBO of the Transformer</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p">The performance of AR-LLM can be analyzed from variational inference perspective. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib90" title="">90</a>]</cite>, <math alttext="J" class="ltx_Math" display="inline" id="S5.SS3.p1.m1" intent=":literal"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> is introduced as a latent variable defined on <math alttext="\{1,\ldots,T\}" class="ltx_Math" display="inline" id="S5.SS3.p1.m2" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1,\ldots,T\}</annotation></semantics></math>. <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="S5.SS3.p1.m3" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> can then be seen as the probability that choosing the position <math alttext="J=j" class="ltx_Math" display="inline" id="S5.SS3.p1.m4" intent=":literal"><semantics><mrow><mi>J</mi><mo>=</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">J=j</annotation></semantics></math>. Thus, the prediction of <math alttext="U_{t}" class="ltx_Math" display="inline" id="S5.SS3.p1.m5" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math> in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S5.E46" title="In Theorem 5 ‚Ä£ V-B Transformer Architecture ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">46</span></a>) is the expectation over <math alttext="J" class="ltx_Math" display="inline" id="S5.SS3.p1.m6" intent=":literal"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E54">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\{\mathbf{A}\mathbf{u}_{J}\}\right),\quad t=n+1,\ldots,T," class="ltx_math_unparsed" display="block" id="S5.E54.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>ùêÆ</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">‚Å°</mo><mi>softmax</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi mathvariant="normal">Œû</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><msub><mi>ùêÄùêÆ</mi><mi>J</mi></msub><mo stretchy="false">}</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{u}_{t}=\arg\mathrm{softmax}\left(\frac{1}{\Xi}\tilde{\mathbf{u}}_{1:N}^{T}\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\{\mathbf{A}\mathbf{u}_{J}\}\right),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(54)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E55">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A},\mathbf{B}\})=\pi_{tj},\quad j=1,\ldots,t-1." class="ltx_Math" display="block" id="S5.E55.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub></mrow><mo rspace="1.167em">,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A},\mathbf{B}\})=\pi_{tj},\quad j=1,\ldots,t-1.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(55)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By applying the principle of variational inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib51" title="">51</a>]</cite>, we then have the following theorems.</p>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm6">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 6</span></span></h6>
<div class="ltx_para" id="Thmthm6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The pre-training phase of Transformer is equivalent to</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E56">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\mathbf{A},\mathbf{B}}\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})),\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S5.E56.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>ELBO</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mi>t</mi></mrow><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\mathbf{A},\mathbf{B}}\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(56)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p">In the pre-training phase, we will maximize the following cross-entropy loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E57">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\Phi}H(P_{t}^{\hbar},Q_{t}^{\Phi})=\min_{\Phi}\mathbb{E}_{P_{t}^{\hbar}}\{\log Q_{t}^{\Phi}\},\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S5.E57.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi mathvariant="normal">Œ¶</mi></munder><mo lspace="0.167em">‚Å°</mo><mi>H</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>min</mi><mi mathvariant="normal">Œ¶</mi></munder><mo lspace="0.167em">‚Å°</mo><msub><mi>ùîº</mi><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><msubsup><mi>Q</mi><mi>t</mi><mi mathvariant="normal">Œ¶</mi></msubsup></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\Phi}H(P_{t}^{\hbar},Q_{t}^{\Phi})=\min_{\Phi}\mathbb{E}_{P_{t}^{\hbar}}\{\log Q_{t}^{\Phi}\},\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(57)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the optimum, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E58">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t}^{\Phi^{\hbar}}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})=P_{t}^{\hbar}." class="ltx_Math" display="block" id="S5.E58.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>Q</mi><mi>t</mi><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></msubsup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mi mathvariant="normal">‚Ñè</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msubsup><mi>P</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Q_{t}^{\Phi^{\hbar}}=P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar})=P(U_{t}^{\hbar}|U_{n+1:t-1}^{\hbar},S_{1:n})=P_{t}^{\hbar}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(58)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, the pre-training phase is equivalent to solve the following optimization problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E59">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\Phi}\log P(U_{t}^{h}|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)." class="ltx_Math" display="block" id="S5.E59.m1" intent=":literal"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mi mathvariant="normal">Œ¶</mi></munder><mo lspace="0.167em">‚Å°</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi>h</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\Phi}\log P(U_{t}^{h}|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(59)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">According to the principle of variational inference, we have</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S5.E60">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E60X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log P(U_{t}^{h}|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)" class="ltx_Math" display="inline" id="S5.E60X.m2" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi>h</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log P(U_{t}^{h}|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="5"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(60)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E60Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E60Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\sum_{j=1}^{t-1}P(U_{t}^{h},j|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)" class="ltx_Math" display="inline" id="S5.E60Xa.m3" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi>h</mi></msubsup><mo>,</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\sum_{j=1}^{t-1}P(U_{t}^{h},j|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E60Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E60Xb.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\sum_{j=1}^{t-1}P(U_{t}^{h},j|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)\frac{Q(j|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}{Q(j|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}" class="ltx_Math" display="inline" id="S5.E60Xb.m3" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi>h</mi></msubsup><mo>,</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mstyle displaystyle="true"><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\sum_{j=1}^{t-1}P(U_{t}^{h},j|U_{n+1:t-1}^{\hbar},S_{1:n};\Phi)\frac{Q(j|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}{Q(j|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E60Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E60Xc.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\left\{\frac{P(U_{t}^{\hbar},J|U_{n+1:t-1}^{\hbar},S_{1:n})}{Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\right\}" class="ltx_math_unparsed" display="inline" id="S5.E60Xc.m3" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\left\{\frac{P(U_{t}^{\hbar},J|U_{n+1:t-1}^{\hbar},S_{1:n})}{Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E60Xd">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\geq" class="ltx_Math" display="inline" id="S5.E60Xd.m2" intent=":literal"><semantics><mo>‚â•</mo><annotation encoding="application/x-tex">\displaystyle\geq</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\left\{\log\frac{P(U_{t}^{\hbar},J|U_{n+1:t-1}^{\hbar},S_{1:n})}{Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\right\}." class="ltx_math_unparsed" display="inline" id="S5.E60Xd.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\left\{\log\frac{P(U_{t}^{\hbar},J|U_{n+1:t-1}^{\hbar},S_{1:n})}{Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The last term is exactly the ELBO, which can be rewritten as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S5.E61">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E61X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\}))" class="ltx_Math" display="inline" id="S5.E61X.m2" intent=":literal"><semantics><mrow><mi>ELBO</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(61)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E61Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E61Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\{\log P(U_{t}^{\hbar},J|U_{n+1:T}^{\hbar},S_{1:n})\}" class="ltx_math_unparsed" display="inline" id="S5.E61Xa.m3" intent=":literal"><semantics><mrow><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})}\{\log P(U_{t}^{\hbar},J|U_{n+1:T}^{\hbar},S_{1:n})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E61Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle-D_{KL}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})\|P(J|U_{n+1:t-1}^{\hbar},S_{1:n}))." class="ltx_math_unparsed" display="inline" id="S5.E61Xb.m2" intent=":literal"><semantics><mrow><mo>‚àí</mo><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mi>J</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.167em">‚à•</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>J</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle-D_{KL}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})\|P(J|U_{n+1:t-1}^{\hbar},S_{1:n})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">As a result, the training phase is equivalent to</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E62">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\mathbf{A},\mathbf{B}}\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})),\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S5.E62.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>ELBO</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mi>t</mi></mrow><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\mathbf{A},\mathbf{B}}\mathrm{ELBO}(Q(J|U_{n+1:t-1}^{\hbar},S_{1:n};\{\mathbf{A},\mathbf{B}\})),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(62)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">‚àé</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm7">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 7</span></span></h6>
<div class="ltx_para" id="Thmthm7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The inference phase of Transformer is equivalent to</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E63">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{U_{t}\in\mathcal{S}(\Omega)}\mathrm{ELBO}(Q_{t}(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})),\quad t=n+1,\ldots,T," class="ltx_Math" display="block" id="S5.E63.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>‚àà</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>ELBO</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mi>t</mi></mrow><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{U_{t}\in\mathcal{S}(\Omega)}\mathrm{ELBO}(Q_{t}(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})),\quad t=n+1,\ldots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(63)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\mathbf{A}^{\hbar+}" class="ltx_Math" display="inline" id="Thmthm7.p1.m1" intent=":literal"><semantics><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{\hbar+}</annotation></semantics></math> and <math alttext="\mathbf{B}^{\hbar+}" class="ltx_Math" display="inline" id="Thmthm7.p1.m2" intent=":literal"><semantics><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{B}^{\hbar+}</annotation></semantics></math> are the parameter matrices after training.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p">In the inference phase, <math alttext="U_{t}" class="ltx_Math" display="inline" id="S5.SS3.p3.m1" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math> is chosen from <math alttext="\mathcal{S}(\Omega)" class="ltx_Math" display="inline" id="S5.SS3.p3.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}(\Omega)</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E64">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\log P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})" class="ltx_Math" display="block" id="S5.E64.m1" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(64)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is maximized. According to the principle of variational inference, we have</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S5.E65">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E65X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})" class="ltx_Math" display="inline" id="S5.E65X.m2" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log P(U_{t}|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="5"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(65)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E65Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E65Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\sum_{j=1}^{t-1}P(U_{t},j|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})" class="ltx_Math" display="inline" id="S5.E65Xa.m3" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\sum_{j=1}^{t-1}P(U_{t},j|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E65Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E65Xb.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\sum_{j=1}^{t-1}P(U_{t},j|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\frac{Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}{Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}" class="ltx_Math" display="inline" id="S5.E65Xb.m3" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mstyle displaystyle="true"><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\sum_{j=1}^{t-1}P(U_{t},j|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\frac{Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}{Q(j|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E65Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E65Xc.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\left\{\frac{P(U_{t},J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\right\}" class="ltx_math_unparsed" display="inline" id="S5.E65Xc.m3" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\left\{\frac{P(U_{t},J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E65Xd">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\geq" class="ltx_Math" display="inline" id="S5.E65Xd.m2" intent=":literal"><semantics><mo>‚â•</mo><annotation encoding="application/x-tex">\displaystyle\geq</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\left\{\log\frac{P(U_{t},J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\right\}." class="ltx_math_unparsed" display="inline" id="S5.E65Xd.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\left\{\log\frac{P(U_{t},J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})}{Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The last term is exactly the ELBO, which can be rewritten as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S5.E66">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E66X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathrm{ELBO}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\}))" class="ltx_Math" display="inline" id="S5.E66X.m2" intent=":literal"><semantics><mrow><mi>ELBO</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{ELBO}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(66)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E66Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S5.E66Xa.m2" intent=":literal"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\{\log P(U_{t}|J,U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\}" class="ltx_math_unparsed" display="inline" id="S5.E66Xa.m3" intent=":literal"><semantics><mrow><msub><mi>ùîº</mi><mrow><mi>J</mi><mo>‚àº</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><mi>J</mi><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{J\sim Q(\cdot|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})}\{\log P(U_{t}|J,U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E66Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle-D_{KL}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})\|P(J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+}))." class="ltx_math_unparsed" display="inline" id="S5.E66Xb.m2" intent=":literal"><semantics><mrow><mo>‚àí</mo><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><mi>J</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.167em">‚à•</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>J</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><msup><mi mathvariant="normal">Œ¶</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle-D_{KL}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})\|P(J|U_{n+1:t-1},S_{1:n};\Phi^{\hbar+})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">As a result, the inference phase is equivalent to</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E67">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{U_{t}\in\mathcal{S}(\Omega)}\mathrm{ELBO}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})),\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S5.E67.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>‚àà</mo><mrow><mi class="ltx_font_mathcaligraphic">ùíÆ</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ©</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">‚Å°</mo><mi>ELBO</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo fence="false">|</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>;</mo><mrow><mo stretchy="false">{</mo><msup><mi>ùêÄ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo>,</mo><msup><mi>ùêÅ</mi><mrow><mi mathvariant="normal">‚Ñè</mi><mo>+</mo></mrow></msup><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mi>t</mi></mrow><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{U_{t}\in\mathcal{S}(\Omega)}\mathrm{ELBO}(Q(J|U_{n+1:t-1},S_{1:n};\{\mathbf{A}^{\hbar+},\mathbf{B}^{\hbar+}\})),\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(67)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">‚àé</p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-D </span><span class="ltx_text ltx_font_italic">Generalization Error Bound of the Transformer</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p">Rademacher complexity and Talagrand‚Äôs concentration inequalities are fundamental tools in statistical learning theory for analyzing the generalization error bounds of machine learning algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib52" title="">52</a>]</cite>. This section applies these tools to study the generalization error bound of the Transformer.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p">Let <math alttext="\mathbf{u}_{t}^{\hbar}" class="ltx_Math" display="inline" id="S5.SS4.p2.m1" intent=":literal"><semantics><msubsup><mi>ùêÆ</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">\mathbf{u}_{t}^{\hbar}</annotation></semantics></math> be the ground-truth output vector at time <math alttext="t" class="ltx_Math" display="inline" id="S5.SS4.p2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> for <math alttext="t=n+1,\ldots,T" class="ltx_Math" display="inline" id="S5.SS4.p2.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=n+1,\ldots,T</annotation></semantics></math>, where the corresponding random variable is <math alttext="U_{t}^{\hbar}" class="ltx_Math" display="inline" id="S5.SS4.p2.m4" intent=":literal"><semantics><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">U_{t}^{\hbar}</annotation></semantics></math>. Therefore, the generalization error is given By</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E68">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(P(U_{t}^{\hbar}),Q(U_{t}))," class="ltx_Math" display="block" id="S5.E68.m1" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">H(P(U_{t}^{\hbar}),Q(U_{t})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(68)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="P(U_{t}^{\hbar})" class="ltx_Math" display="inline" id="S5.SS4.p2.m5" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(U_{t}^{\hbar})</annotation></semantics></math> is the one-shot coding, <math alttext="Q(U_{t})" class="ltx_Math" display="inline" id="S5.SS4.p2.m6" intent=":literal"><semantics><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(U_{t})</annotation></semantics></math> is the output of the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS4.p2.m7" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> function. Given <math alttext="t" class="ltx_Math" display="inline" id="S5.SS4.p2.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, we take <math alttext="M" class="ltx_Math" display="inline" id="S5.SS4.p2.m9" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> samples from the Transformer output <math alttext="U_{t}" class="ltx_Math" display="inline" id="S5.SS4.p2.m10" intent=":literal"><semantics><msub><mi>U</mi><mi>t</mi></msub><annotation encoding="application/x-tex">U_{t}</annotation></semantics></math>, each of which is denoted as <math alttext="\mathbf{u}_{mt}" class="ltx_Math" display="inline" id="S5.SS4.p2.m11" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{u}_{mt}</annotation></semantics></math> for <math alttext="m=1,\ldots,M" class="ltx_Math" display="inline" id="S5.SS4.p2.m12" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>M</mi></mrow></mrow><annotation encoding="application/x-tex">m=1,\ldots,M</annotation></semantics></math>. Recalling Theorem <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmthm5" title="Theorem 5 ‚Ä£ V-B Transformer Architecture ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">5</span></a>, the <math alttext="i" class="ltx_Math" display="inline" id="S5.SS4.p2.m13" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th entry of the logits <math alttext="\mathbf{z}_{m}" class="ltx_Math" display="inline" id="S5.SS4.p2.m14" intent=":literal"><semantics><msub><mi>ùê≥</mi><mi>m</mi></msub><annotation encoding="application/x-tex">\mathbf{z}_{m}</annotation></semantics></math> is defined by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E69">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{m}^{i}=\frac{1}{\Xi}\tilde{\mathbf{u}}_{i}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{A}\mathbf{u}_{mj}\right),\quad i=1,\ldots,N." class="ltx_Math" display="block" id="S5.E69.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>z</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mfrac><mn>1</mn><mi mathvariant="normal">Œû</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mover accent="true"><mi>ùêÆ</mi><mo>~</mo></mover><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÄùêÆ</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>N</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">z_{m}^{i}=\frac{1}{\Xi}\tilde{\mathbf{u}}_{i}^{T}\left(\sum_{j=1}^{t-1}\pi_{tj}\mathbf{A}\mathbf{u}_{mj}\right),\quad i=1,\ldots,N.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(69)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The empirical generalization error over a sample set with size <math alttext="M" class="ltx_Math" display="inline" id="S5.SS4.p2.m15" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E70">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})=\frac{1}{M}\sum_{m=1}^{M}\mathbf{1}^{T}(\mathbf{u}_{mt}^{\hbar})\log\frac{1}{\mathbf{q}(\mathbf{z}_{m})}=\frac{1}{M}\sum_{m=1}^{M}\log\frac{1}{q(z_{m}^{\hbar})}," class="ltx_Math" display="block" id="S5.E70.m1" intent=":literal"><semantics><mrow><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msup><mn>ùüè</mn><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>ùêÆ</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mn>1</mn><mrow><mi>ùê™</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê≥</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mn>1</mn><mrow><mi>q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})=\frac{1}{M}\sum_{m=1}^{M}\mathbf{1}^{T}(\mathbf{u}_{mt}^{\hbar})\log\frac{1}{\mathbf{q}(\mathbf{z}_{m})}=\frac{1}{M}\sum_{m=1}^{M}\log\frac{1}{q(z_{m}^{\hbar})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(70)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{q}(\mathbf{z}_{m})" class="ltx_Math" display="inline" id="S5.SS4.p2.m16" intent=":literal"><semantics><mrow><mi>ùê™</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê≥</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}(\mathbf{z}_{m})</annotation></semantics></math> is the output of the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS4.p2.m17" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> function, and</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E71">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q(z_{m}^{\hbar})=\mathbf{1}^{T}(\mathbf{u}_{mt}^{\hbar})\mathbf{q}(\mathbf{z}_{m})." class="ltx_Math" display="block" id="S5.E71.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mn>ùüè</mn><mi>T</mi></msup><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>ùêÆ</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mi>ùê™</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê≥</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">q(z_{m}^{\hbar})=\mathbf{1}^{T}(\mathbf{u}_{mt}^{\hbar})\mathbf{q}(\mathbf{z}_{m}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(71)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_thm" id="Thmthm8">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 8</span></span></h6>
<div class="ltx_para" id="Thmthm8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For any <math alttext="\delta&gt;0" class="ltx_Math" display="inline" id="Thmthm8.p1.m1" intent=":literal"><semantics><mrow><mi>Œ¥</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta&gt;0</annotation></semantics></math>, the generalization error of the Transformer is upper bounded by</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E72">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(P(U_{t}^{\hbar}),Q(U_{t}))\leq\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})+\frac{2\sqrt{2}}{M}\sum_{m=1}^{M}|z_{m}^{\hbar}|+3\sqrt{\frac{\log\frac{2}{\delta}}{2M}},\quad t=n+1,\ldots,T." class="ltx_Math" display="block" id="S5.E72.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚â§</mo><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mrow><mn>2</mn><mo lspace="0em" rspace="0em">‚Äã</mo><msqrt><mn>2</mn></msqrt></mrow><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false" rspace="0em">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mo stretchy="false">|</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">‚Äã</mo><msqrt><mfrac><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mn>2</mn><mi>Œ¥</mi></mfrac></mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mi>M</mi></mrow></mfrac></msqrt></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(P(U_{t}^{\hbar}),Q(U_{t}))\leq\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})+\frac{2\sqrt{2}}{M}\sum_{m=1}^{M}|z_{m}^{\hbar}|+3\sqrt{\frac{\log\frac{2}{\delta}}{2M}},\quad t=n+1,\ldots,T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(72)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">with probability at least <math alttext="1-\delta" class="ltx_Math" display="inline" id="Thmthm8.p1.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo>‚àí</mo><mi>Œ¥</mi></mrow><annotation encoding="application/x-tex">1-\delta</annotation></semantics></math> over the choice of <math alttext="M" class="ltx_Math" display="inline" id="Thmthm8.p1.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> samples.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_font_italic ltx_title_proof">Proof:</h6>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p">The empirical Rademacher complexity of the Transformer is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E73">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})=\mathbb{E}_{\bm{\sigma}}\left\{\sup_{\mathbf{A},\mathbf{B}}\frac{1}{M}\sum_{m=1}^{M}\sigma_{m}\log\frac{1}{q(z_{m}^{\hbar})}\right\}," class="ltx_Math" display="block" id="S5.E73.m1" intent=":literal"><semantics><mrow><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñõ</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>ùîº</mi><mi>ùùà</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mrow><munder><mo lspace="0em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi></mrow></munder><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>œÉ</mi><mi>m</mi></msub><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mn>1</mn><mrow><mi>q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})=\mathbb{E}_{\bm{\sigma}}\left\{\sup_{\mathbf{A},\mathbf{B}}\frac{1}{M}\sum_{m=1}^{M}\sigma_{m}\log\frac{1}{q(z_{m}^{\hbar})}\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(73)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{\sigma}" class="ltx_Math" display="inline" id="S5.SS4.p3.m1" intent=":literal"><semantics><mi>ùùà</mi><annotation encoding="application/x-tex">\bm{\sigma}</annotation></semantics></math> is a Rademacher sequence. According to Theorem 3.3 in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib52" title="">52</a>]</cite>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E74">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(P(U_{t}^{\hbar}),Q(U_{t}))\leq\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})+2\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})+3\sqrt{\frac{\log\frac{2}{\delta}}{2M}}." class="ltx_Math" display="block" id="S5.E74.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>U</mi><mi>t</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚â§</mo><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñõ</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">‚Äã</mo><msqrt><mfrac><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mfrac><mn>2</mn><mi>Œ¥</mi></mfrac></mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mi>M</mi></mrow></mfrac></msqrt></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(P(U_{t}^{\hbar}),Q(U_{t}))\leq\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})+2\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})+3\sqrt{\frac{\log\frac{2}{\delta}}{2M}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(74)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Because <math alttext="\mathbf{q}(\mathbf{z}_{m})" class="ltx_Math" display="inline" id="S5.SS4.p3.m2" intent=":literal"><semantics><mrow><mi>ùê™</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>ùê≥</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{q}(\mathbf{z}_{m})</annotation></semantics></math> is the output of the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS4.p3.m3" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> function, <math alttext="\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})" class="ltx_Math" display="inline" id="S5.SS4.p3.m4" intent=":literal"><semantics><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}(\mathbf{A},\mathbf{B})</annotation></semantics></math> is <math alttext="\sqrt{2}" class="ltx_Math" display="inline" id="S5.SS4.p3.m5" intent=":literal"><semantics><msqrt><mn>2</mn></msqrt><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math>-Lipschitz over <math alttext="z_{m}^{\hbar}" class="ltx_Math" display="inline" id="S5.SS4.p3.m6" intent=":literal"><semantics><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><annotation encoding="application/x-tex">z_{m}^{\hbar}</annotation></semantics></math> for <math alttext="l^{2}" class="ltx_Math" display="inline" id="S5.SS4.p3.m7" intent=":literal"><semantics><msup><mi>l</mi><mn>2</mn></msup><annotation encoding="application/x-tex">l^{2}</annotation></semantics></math>-norm. According to Talagrand‚Äôs Lemma in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib52" title="">52</a>]</cite>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E75">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})\leq\mathbb{E}_{\bm{\sigma}}\left\{\sup_{\mathbf{A},\mathbf{B}}\frac{1}{M}\sum_{m=1}^{M}\sigma_{m}z_{m}^{\hbar}\right\}\leq\frac{\sqrt{2}}{M}\sum_{m=1}^{M}|z_{m}^{\hbar}|." class="ltx_Math" display="block" id="S5.E75.m1" intent=":literal"><semantics><mrow><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">‚Ñõ</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚â§</mo><mrow><msub><mi>ùîº</mi><mi>ùùà</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mrow><munder><mo lspace="0em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mi>ùêÄ</mi><mo>,</mo><mi>ùêÅ</mi></mrow></munder><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>œÉ</mi><mi>m</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>‚â§</mo><mrow><mfrac><msqrt><mn>2</mn></msqrt><mi>M</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false" rspace="0em">‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mo stretchy="false">|</mo><msubsup><mi>z</mi><mi>m</mi><mi mathvariant="normal">‚Ñè</mi></msubsup><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{R}}(\mathbf{A},\mathbf{B})\leq\mathbb{E}_{\bm{\sigma}}\left\{\sup_{\mathbf{A},\mathbf{B}}\frac{1}{M}\sum_{m=1}^{M}\sigma_{m}z_{m}^{\hbar}\right\}\leq\frac{\sqrt{2}}{M}\sum_{m=1}^{M}|z_{m}^{\hbar}|.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(75)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This theorem has been established.
‚àé</p>
</div>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p">This result shows that the logits determines the accuracy during the inference phase. Therefore, when using quantization for inference acceleration, it is crucial to ensure that the quantization algorithm has a minimal impact on the logits.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-E </span><span class="ltx_text ltx_font_italic">Memory Capacity of the Transformer</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p">The statistical physics approaches, such as spin glass model and replica method, have been widely used to analyze the performance of signal processing, coding, and satisfiability (SAT) problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib91" title="">91</a>]</cite>. In a series of landmark papers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib55" title="">55</a>]</cite>, Gardner investigated the memory capacity of the classical Hopfield network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib92" title="">92</a>]</cite> by applying the replica method, which is referred to as Gardner capacity afterwards.</p>
</div>
<div class="ltx_theorem ltx_theorem_defn" id="Thmdefn14">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 14</span></span></h6>
<div class="ltx_para" id="Thmdefn14.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="N_{P}" class="ltx_Math" display="inline" id="Thmdefn14.p1.m1" intent=":literal"><semantics><msub><mi>N</mi><mi>P</mi></msub><annotation encoding="application/x-tex">N_{P}</annotation></semantics></math> be the maximum number of random patterns which can be memorized in a classical Hopfield network with <math alttext="n" class="ltx_Math" display="inline" id="Thmdefn14.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons. The generalized Gardner capacity is defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E76">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C_{G}=\frac{\alpha(N_{P})}{n}," class="ltx_Math" display="block" id="S5.E76.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>C</mi><mi>G</mi></msub><mo>=</mo><mfrac><mrow><mi>Œ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><msub><mi>N</mi><mi>P</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mi>n</mi></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">C_{G}=\frac{\alpha(N_{P})}{n},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(76)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\alpha(\cdot)" class="ltx_Math" display="inline" id="Thmdefn14.p1.m3" intent=":literal"><semantics><mrow><mi>Œ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha(\cdot)</annotation></semantics></math> is chosen to scale with <math alttext="n" class="ltx_Math" display="inline" id="Thmdefn14.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. It is an identity function in the original definition.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p">As a matter of fact, generalized Gardner capacity has a deep connection with Shannon capacity. If the pattern here is not a binary <math alttext="n" class="ltx_Math" display="inline" id="S5.SS5.p2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-sequence but a binary <math alttext="n" class="ltx_Math" display="inline" id="S5.SS5.p2.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-sphere, the Gardner capacity is equivalent to Shannon capacity, where <math alttext="\alpha(\cdot)" class="ltx_Math" display="inline" id="S5.SS5.p2.m3" intent=":literal"><semantics><mrow><mi>Œ±</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha(\cdot)</annotation></semantics></math> is chosen as a logarithm function. The transformation from <math alttext="n" class="ltx_Math" display="inline" id="S5.SS5.p2.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-sequence to <math alttext="n" class="ltx_Math" display="inline" id="S5.SS5.p2.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-sphere is critical, which explains the error correction capability of modern neural networks.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p">Recent work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib93" title="">93</a>]</cite> focused on the modern continuous Hopfield network, which is shown to be equivalent to the attention scheme. It is also proved that the memory capacity is exponential in the dimension of the space of the query and key-value patterns. Therefore, it is not surprising that a large amount of patterns can be memorized by a small LLM. Following this idea, we model the behavior of Transformers with associative memories using modern continuous Hopfield networks, which is used to explain the scaling law from theoretic perspective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib94" title="">94</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-F </span><span class="ltx_text ltx_font_italic">Semantic Information Theoretic Measure for the Transformer</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p">In Section <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#S3" title="III LLM as a Next-token Predictor ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">III</span></a>, we introduce semantic information theoretic measures for LLMs, such as the directed rate-distortion function in the pre-training phase, the directed rate-reward function in the post-training phase, and the semantic information flow in inference phase, where the key is to estimate the directed information.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p">The directed information <math alttext="I(S_{1:n}\to U_{n+1:t};\Phi)" class="ltx_Math" display="inline" id="S5.SS6.p2.m1" intent=":literal"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><mrow><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(S_{1:n}\to U_{n+1:t};\Phi)</annotation></semantics></math> can be represented by KL divergence as follows</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E77">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(S_{1:n}\to U_{n+1:t};\Phi)=D_{KL}\left(P(S_{1:n},U_{n+1,t})\|P(S_{1:n})\prod_{j=n+1}^{t}P(U_{j}|U_{n+1:j})\right)." class="ltx_math_unparsed" display="block" id="S5.E77.m1" intent=":literal"><semantics><mrow><mi>I</mi><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">‚Üí</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo>;</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>L</mi></mrow></msub><mrow><mo>(</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.167em">‚à•</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><munderover><mo movablelimits="false">‚àè</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>t</mi></munderover><mi>P</mi><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><mi>j</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>U</mi><mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(S_{1:n}\to U_{n+1:t};\Phi)=D_{KL}\left(P(S_{1:n},U_{n+1,t})\|P(S_{1:n})\prod_{j=n+1}^{t}P(U_{j}|U_{n+1:j})\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(77)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, the Donsker-Varadhan representation can be used for directed information estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib95" title="">95</a>]</cite>. This idea is proposed and thoroughly analyzed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib96" title="">96</a>]</cite> for transfer entropy estimation, where the transformer itself is used as the estimator.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-G </span><span class="ltx_text ltx_font_italic">Other Architectures</span>
</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p">To simplify the computation complexity in both training and inference phases. Various LLM architectures, such as Mamba/Mamba2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib57" title="">57</a>]</cite> and LLaDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib58" title="">58</a>]</cite>, have been proposed. We will discuss the relation between these new architectures and Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn13" title="Definition 13 ‚Ä£ V-A TV-VAR based AR-LLMs ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-G1 </span>Mamba/Mamba2</h4>
<div class="ltx_para" id="S5.SS7.SSS1.p1">
<p class="ltx_p">To save the computation of <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS7.SSS1.p1.m1" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> in attention scheme, Mamba/ Mamba2 architectures are proposed and thoroughly analyzed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib57" title="">57</a>]</cite>. Inspired by control theory, the discrete state space model (SSM) used in Mamba/Mamba2 is</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E78">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left\{\begin{aligned} \mathbf{u}_{t}&amp;=\mathbf{A}_{t}\mathbf{u}_{t-1}+\mathbf{B}_{t}\mathbf{s}_{t};\\
\mathbf{y}_{t}&amp;=\mathbf{C}\mathbf{u}_{t}.\end{aligned}\right." class="ltx_math_unparsed" display="block" id="S5.E78.m1" intent=":literal"><semantics><mrow><mo>{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>ùêÆ</mi><mi>t</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msub><mi>ùêÄ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùêÆ</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>ùêÅ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><msub><mi>ùê¨</mi><mi>t</mi></msub></mrow></mrow></mrow><mo>;</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>ùê≤</mi><mi>t</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>=</mo><msub><mi>ùêÇùêÆ</mi><mi>t</mi></msub></mrow><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\left\{\begin{aligned} \mathbf{u}_{t}&amp;=\mathbf{A}_{t}\mathbf{u}_{t-1}+\mathbf{B}_{t}\mathbf{s}_{t};\\
\mathbf{y}_{t}&amp;=\mathbf{C}\mathbf{u}_{t}.\end{aligned}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(78)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Clearly, the SSM is a special case of the AR-LLM in Definition <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#Thmdefn13" title="Definition 13 ‚Ä£ V-A TV-VAR based AR-LLMs ‚Ä£ V Autoregression LLMs ‚Ä£ Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"><span class="ltx_text ltx_ref_tag">13</span></a>, which exactly belongs to linear TV-VAR models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib50" title="">50</a>]</cite>. The linear TV-VAR model is widely used in time series analysis for economics and finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib98" title="">98</a>]</cite>. Therefore, the developed parameter estimation method may be applicable to improve the performance of Mamba/Mamba2. Because there lacks the bilinear model of semantic relevance, it is not difficult to understand that the performance of Mamba/Mamba2 could be worse than Transformer. However, the Mamba/Mamba2 architectures inspire us to consider other forms of AR-LLM which may have a similar performance as Transformer but much lower computation complexity. Based on the improved Mamba2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib99" title="">99</a>]</cite>, Qwen3-Next is the first LLM which implements the hybrid attention scheme.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://qwen.ai/blog?from=research.latest-advancements-list&amp;id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;</span></span></span> The Transformer, however, is different from linear TV-VAR model because <math alttext="\pi_{tj}" class="ltx_Math" display="inline" id="S5.SS7.SSS1.p1.m2" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{tj}</annotation></semantics></math> introduces a non-linear relation, i.e., the <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S5.SS7.SSS1.p1.m3" intent=":literal"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math> function over a bilinear form of <math alttext="\mathbf{u}_{t}" class="ltx_Math" display="inline" id="S5.SS7.SSS1.p1.m4" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{t}</annotation></semantics></math> and <math alttext="\mathbf{u}_{j}" class="ltx_Math" display="inline" id="S5.SS7.SSS1.p1.m5" intent=":literal"><semantics><msub><mi>ùêÆ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathbf{u}_{j}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-G2 </span>LLaDA</h4>
<div class="ltx_para" id="S5.SS7.SSS2.p1">
<p class="ltx_p">As a diffusion LLM, LLaDA constitutes a groundbreaking attempt to transcend the Transformer paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib58" title="">58</a>]</cite>. In LLaDA, it assumes many tokens in an utterance are masked, which will be predicted based on the unmasked ones. The loss function for training LLaDA is a cross-entropy computed only on the masked tokens:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E79">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\Phi)=-\mathbb{E}_{\tau,U_{1:T}^{\tau},U_{1:T}^{0}}\left\{\frac{1}{\tau}\sum_{t=1}^{T}\mathbf{1}(U_{t}^{\tau}=M)\log P(U_{t}^{0}|U_{1:T}^{\tau};\Phi)\right\}," class="ltx_Math" display="block" id="S5.E79.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Œ¶</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>‚àí</mo><mrow><msub><mi>ùîº</mi><mrow><mi>œÑ</mi><mo>,</mo><msubsup><mi>U</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi>œÑ</mi></msubsup><mo>,</mo><msubsup><mi>U</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mn>0</mn></msubsup></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>{</mo><mrow><mfrac><mn>1</mn><mi>œÑ</mi></mfrac><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mi>œÑ</mi></msubsup><mo>=</mo><mi>M</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">‚Äã</mo><mrow><mi>log</mi><mo lspace="0.167em">‚Å°</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>U</mi><mi>t</mi><mn>0</mn></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>U</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>T</mi></mrow><mi>œÑ</mi></msubsup><mo>;</mo><mi mathvariant="normal">Œ¶</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\Phi)=-\mathbb{E}_{\tau,U_{1:T}^{\tau},U_{1:T}^{0}}\left\{\frac{1}{\tau}\sum_{t=1}^{T}\mathbf{1}(U_{t}^{\tau}=M)\log P(U_{t}^{0}|U_{1:T}^{\tau};\Phi)\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(79)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="M" class="ltx_Math" display="inline" id="S5.SS7.SSS2.p1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> denote the masked token. The transformer without causal mask is used as the core component to predict the masked tokens. Evidently, while LLaDA is fundamentally built upon a diffusion framework, the AR-LLM remains central to the task of masked token prediction in LLaDA.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">Drawing from the theory of rate-distortion function, directed information, and Granger causality, this paper aims to uncover the semantic information-theoretic principles underlying LLMs. We discussed the structure-agnostic information-theoretic measures, the token-level semantic embedding, and the general definition of AR-LLM, from which the Transformer architecture and its performance have been derived theoretically. Our theory indicates that the capabilities of current LLMs remain within the scope of Granger causality. How to achieve the counterfactual reasoning and system 2 reasoning abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.01202v1#bib.bib101" title="">101</a>]</cite>, remains a formidable challenge. Consequently, our semantic information theory framework provides a lens through which many experimentally observations can be explained, which also paves the way for unlocking the full potential of LLMs.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p">I am grateful to T. Wu, X. Niu, K. Zhang, C. Zhang, Y. Lan, Z. Zhong, B. Chen, and Q. Zhang for productive discussions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> C. Shannon, ‚ÄúA mathematical theory of communication,‚Äù <em class="ltx_emph ltx_font_italic">Bell System Technical Journal</em>, vol. 27, no. 7, pp. 379-423, Oct. 1948.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> W. Weaver, ‚ÄúRecent contributions to the mathematical theory of communications,‚Äù <em class="ltx_emph ltx_font_italic">The Rockefeller Foundation</em>, Sep. 1949.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> R. Carnap, ‚ÄúEmpiricism, semantics, and ontology,‚Äù <em class="ltx_emph ltx_font_italic">Revue Internationale de Philosophie</em>, no. 4, pp. 20-40, Apr. 1950.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> R. Carnap and Y. Bar-Hillel, ‚ÄúAn outline of a theory of semantic information,‚Äù Massachusetts Institute of Technology, Cambridge, MA, USA, Research Laboratory of Electronics Technical Report No. 247, Oct. 1952.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Y. Bar-Hillel and R. Carnap, ‚ÄúSemantic information,‚Äù <em class="ltx_emph ltx_font_italic">The British Journal for the Philosophy of Science</em>, vol. 4, no. 14, pp. 147-157, Aug. 1953.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> R. Carnap, <em class="ltx_emph ltx_font_italic">Meaning and Necessity: A Study in Semantics and Modal Logic</em>, 2nd ed. Chicago, IL, USA: University of Chicago Press, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> M. Burgin, <em class="ltx_emph ltx_font_italic">Theory of Information: Fundamentality, Diversity and Unification</em>. Singapore: World Scientific Publishing, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> L. Floridi, Ed., <em class="ltx_emph ltx_font_italic">The Routledge Handbook of Philosophy of Information</em>. London, UK: Routledge, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> R. Solomonoff, ‚ÄúA formal theory of inductive inference - Part 1,‚Äù <em class="ltx_emph ltx_font_italic">Information and Control</em>, vol. 7, no. 1, pp. 1-22, Mar. 1964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> R. Solomonoff, ‚ÄúA formal theory of inductive inference - Part 2,‚Äù <em class="ltx_emph ltx_font_italic">Information and Control</em>, vol. 7, no. 2, pp. 224-254, Jun. 1964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> R. Solomonoff, ‚ÄúThe discovery of algorithmic probability,‚Äù <em class="ltx_emph ltx_font_italic">Journal of Computer and System Sciences</em>, vol. 55, no. 1, pp. 73-88, Aug. 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> A. Kolmogorov, ‚ÄúThree approaches to the quantitative definition of information,‚Äù <em class="ltx_emph ltx_font_italic">International Journal of Computer Mathematics</em>, vol. 2, no. 1-4, pp. 157-168, Jan. 1968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> A. Kolmogorov, ‚ÄúLogical basis for information theory and probability theory,‚Äù <em class="ltx_emph ltx_font_italic">IEEE Trans. Inf. Theory</em>, vol. 14, no. 5, pp. 662-664, Sep. 1968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> M. Hutter, <em class="ltx_emph ltx_font_italic">Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability</em>. Berlin, Germany: Springer, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> A. Shen, V. Uspensky, and N. Vereshchagin, <em class="ltx_emph ltx_font_italic">Kolmogorov Complexity and Algorithmic Randomness</em>. Providence, RI, USA: American Mathematical Society, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> T. Cover and J. Thomas, <em class="ltx_emph ltx_font_italic">Elements of Information Theory</em>, 2nd ed. Hoboken, NJ, USA: John Wiley &amp; Sons, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> B. Poole, S. Ozair, A. Oord, A. Alemi, and G. Tucker, ‚ÄúOn variational bounds of mutual information,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. 36th ICML ‚Äô19</em>, Long Beach, CA, USA: ICML, Jun. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> R. Sutton, ‚ÄúThe bitter lesson,‚Äù University of Alberta, Edmonton, Canada, Mar. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> H. Luhn, ‚ÄúA new method of recording and searching information,‚Äù <em class="ltx_emph ltx_font_italic">American Documentation</em>, vol. 4, no. 1, pp. 14-16, Jan. 1953.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> G. Salton, A. Wong, and C. Yang, ‚ÄúA vector space model for automatic indexing,‚Äù <em class="ltx_emph ltx_font_italic">Commun. ACM</em>, vol. 18, no. 11, pp. 613-620, Nov. 1975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, ‚ÄúA neural probabilistic language model,‚Äù <em class="ltx_emph ltx_font_italic">J. Machine Learn. Res.</em>, vol. 3, pp. 1137-1155, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfficient estimation of word representations in vector space,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 1301.3781</em>, Sep. 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‚ÄúDistributed representations of words and phrases and their compositionality,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. 27th NIPS ‚Äô13</em>, Lake Tahoe, NV, USA, Dec. 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> J. Pennington, R. Socher, and C. Manning, ‚ÄúGloVe: Global vectors for word representation,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. ACL EMNLP ‚Äô14</em>, Doha, Qatar, Oct. 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‚ÄúEnriching word vectors with subword information,‚Äù <em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 5, pp. 135-146, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> M. Peters et al., ‚ÄúDeep contextualized word representations,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. ACL NAACL-HLT ‚Äô18</em>, New Orleans, LA, USA, Jun. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> D. Jurafsky and J. Martin, <em class="ltx_emph ltx_font_italic">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models</em>, 3rd ed. Draft, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. 31st NIPS ‚Äô17</em>, Long Beach, CA, USA, 4-9 Dec. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‚ÄúImproving language understanding by generative pre-training,‚Äù <em class="ltx_emph ltx_font_italic">OpenAI</em>, Jun. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‚ÄúLanguage models are unsupervised multitask learners,‚Äù <em class="ltx_emph ltx_font_italic">OpenAI</em>, Feb. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> T. Brown et al., ‚ÄúLanguage models are few-shot learners,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. 34th NeurIPS ‚Äô20</em>, Virtual Conference, 6-12 Dec. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> L. Ouyang et al., ‚ÄúTraining language models to follow instructions with human feedback,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2203.02155</em>, Mar. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> D. Guo et al., ‚ÄúDeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning,‚Äù <em class="ltx_emph ltx_font_italic">Nature</em>, vol. 645, no. 8081, pp. 633-638, Sep. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> ‚ÄúDeepSeek-V3.2-Exp: Boosting long-context efficiency with DeepSeek sparse attention,‚Äù <em class="ltx_emph ltx_font_italic">DeepSeek</em>, Hangzhou, China, Sep. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> Y. Polyanskiy and Y. Wu, <em class="ltx_emph ltx_font_italic">Information Theory: From Coding to Learning</em>. Cambridge, UK: Cambridge University Press, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"> R. Shwartz-Ziv and N. Tishby, ‚ÄúOpening the black box of deep neural networks via information,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 1703.00810</em>, Apr. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"> C. Shani, D. Jurafsky, Y. LeCun, and R. Shwartz-Ziv, ‚ÄúFrom tokens to thoughts: How LLMs and humans trade compression for meaning,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2505.17117</em>, Jun. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"> T. Weissman, ‚ÄúToward textual transform coding,‚Äù <em class="ltx_emph ltx_font_italic">IEEE BITS Inform. Theory Mag.</em>, vol. 3, no. 2, pp. 32-40, Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"> X. Niu, B. Bai, N. Guo, W. Zhang, and W. Han, ‚ÄúRate-distortion-perception trade-off in information theory, generative models, and intelligent communications,‚Äù <em class="ltx_emph ltx_font_italic">Entropy</em>, vol. 27, no. 4, Apr. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"> B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet, ‚ÄúA mathematical perspective on transformers,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2312.10794</em>, Aug. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"> M. Rodrigues and Y. Eldar, <em class="ltx_emph ltx_font_italic">Information-Theoretic Methods in Data Science</em>. Cambridge, UK: Cambridge University Press, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"> J. Massey, ‚ÄúCausality, feedback and directed information,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. IEEE ISIT ‚Äô90</em>, Waikiki, HI, USA, Nov. 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"> T. Berger, <em class="ltx_emph ltx_font_italic">Rate Distortion Theory: A Mathematical Basis for Data Compression</em>. Englewood Cliffs, NJ, USA: Prentice Hall PTR, 1971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"> R. Sutton and A. Barto, <em class="ltx_emph ltx_font_italic">Reinforcement Learning: An Introduction</em>, 2nd ed. Cambridge, MA, USA: The MIT Press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"> C. Granger, ‚ÄúTesting for causality: A personal viewpoint,‚Äù <em class="ltx_emph ltx_font_italic">Journal of Economic Dynamics and Control</em>, vol. 2, no. 1, pp. 329-352, Jan. 1980.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"> M. Gromov, <em class="ltx_emph ltx_font_italic">Metric Structures for Riemannian and Non-Riemannian Spaces</em>. Boston, MA, USA: Birkh√§user, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"> C. Villani, <em class="ltx_emph ltx_font_italic">Optimal Transport: Old and New</em>. New York, NY, USA: Springer, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"> A. Oord, Y. Li, and O. Vinyals, ‚ÄúRepresentation learning with contrastive predictive coding,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 1807.03748</em>, Jan. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"> A. Neelakantan et al., ‚ÄúText and code embeddings by contrastive pre-training,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2201.10005</em>, Jan. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"> H. L√ºtkepohl, <em class="ltx_emph ltx_font_italic">New Introduction to Multiple Time Series Analysis</em>. Berlin, Germany: Springer, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"> M. Wainwright and M. Jordan, ‚ÄúGraphical models, exponential families, and variational inference,‚Äù <em class="ltx_emph ltx_font_italic">Foundation and Trends in Machine Learning</em>, vol. 1, no. 1-2, pp. 1-305, Nov. 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"> M. Mohri, A. Rostamizadeh, and A. Talwalkar, <em class="ltx_emph ltx_font_italic">Foundations of Machine Learning</em>, 2nd ed. Cambridge, MA, USA: The MIT Press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"> E. Gardner, ‚ÄúThe space of interactions in neural network models,‚Äù <em class="ltx_emph ltx_font_italic">J. Phys. A: Math. Gen.</em>, vol. 21, no. 1, pp. 257-270, Jan. 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"> E. Gardner and B. Derrida, ‚ÄúOptimal storage properties of neural network models,‚Äù <em class="ltx_emph ltx_font_italic">J. Phys. A: Math. Gen.</em>, vol. 21, no. 1, pp. 271-284, Jan. 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"> E. Gardner and B. Derrida, ‚ÄúThree unfinished works on the optimal storage capacity of networks,‚Äù <em class="ltx_emph ltx_font_italic">J. Phys. A: Math. Gen.</em>, vol. 22, no. 12, pp. 1983-1994, Jun. 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"> A. Gu and T. Dao, ‚ÄúMamba: Linear-time sequence modeling with selective state spaces,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2312.00752</em>, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"> T. Dao and A. Gu, ‚ÄúTransformers are SSMs: Generalized models and efficient algorithms through structured state space duality,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2405.21060</em>, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"> S. Nie et al., ‚ÄúLarge language diffusion models,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2502.09992</em>, Feb. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"> R. Blahut, ‚ÄúComputation of channel capacity and rate-distortion functions,‚Äù <em class="ltx_emph ltx_font_italic">IEEE Trans. Inf. Theory</em>, vol. 18, no. 4, pp. 460-473, Jul. 1972.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"> S. Arimoto, ‚ÄúAn algorithm for computing the capacity of arbitrary discrete memoryless channels,‚Äù <em class="ltx_emph ltx_font_italic">IEEE Trans. Inf. Theory</em>, vol. 18, no. 1, pp. 14-20, Jan. 1972.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"> S. Wu, W. Ye, H. Wu, H. Wu, W. Zhang, and B. Bai, ‚ÄúA communication optimal transport approach to the computation of rate distortion functions,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2212.10098</em>, Dec. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"> L. Chen et al., ‚ÄúA constrained BA algorithm for rate-distortion and distortion-rate functions,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2305.02650</em>, Jan. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"> C. Chen et al., ‚ÄúComputation of rate-distortion-perception functions with Wasserstein barycenter,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. IEEE ISIT ‚Äô23</em>, Taipei, Taiwan, Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"> G. Kramer, ‚ÄúDirected information for channels with feedback,‚Äù Ph. D Dissertation, ETH Zurich, Zurich, Switzerland, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"> R. Dobrushin, ‚ÄúGeneral formulation of Shannon‚Äôs main theorem in information theory,‚Äù <em class="ltx_emph ltx_font_italic">American Mathematical Society Translations: Series 2</em>, vol. 33, no. 2, pp. 323-438, 1963.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"> I. Naiss and H. Permuter, ‚ÄúExtension of the Blahut-Arimoto algorithm for maximizing directed information,‚Äù <em class="ltx_emph ltx_font_italic">IEEE Trans. Inf. Theory</em>, vol. 59, no. 1, pp. 204-222, Jan. 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"> M. Belghazi et al., ‚ÄúMINE: Mutual information neural estimation,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 1801.04062</em>, Aug. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"> D. Tsur, Z. Aharoni, Z. Goldfeld, and H. Permuter, ‚ÄúNeural estimation and optimization of directed information over continuous spaces,‚Äù <em class="ltx_emph ltx_font_italic">IEEE Trans. on Inf. Theory</em>, vol. 69, no. 8, pp. 4777-4798, Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"> V. Strassen, ‚ÄúAsymptotische absch√§tzungen in Shannon‚Äôs informationstheorie,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. Trans. 3rd Prague Conf. Inf. Theory ‚Äô62</em>, Prague, Czech Republic, 1962.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"> P. Amblard and O. Michel, ‚ÄúThe relation between Granger causality and directed information theory: A review,‚Äù <em class="ltx_emph ltx_font_italic">Entropy</em>, vol. 15, no. 1, pp. 113-143, Jan. 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"> T. Schreiber, ‚ÄúMeasuring information transfer,‚Äù <em class="ltx_emph ltx_font_italic">Phys. Rev. Lett.</em>, vol. 85, no. 2, pp. 461-464, Jul. 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"> L. Barnett, A. Barrett, and A. Seth, ‚ÄúGranger causality and transfer entropy are equivalent for Gaussian variables,‚Äù <em class="ltx_emph ltx_font_italic">Phys. Rev. Lett.</em>, vol. 103, no. 23, p. 238701, Dec. 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"> D. Gen√ßaƒüa, Ed., ‚ÄúTransfer entropy,‚Äù <em class="ltx_emph ltx_font_italic">Entropy</em>, vol. 20, no. 4, p. 288, Apr. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"> J. Pearl, <em class="ltx_emph ltx_font_italic">Causality: Models, Reasoning, and Inference</em>, 2nd ed. New York, NY, USA: Cambridge University Press, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"> P. Gr√ºnwald and P. Vit√°nyi, ‚ÄúShannon information and Kolmogorov complexity,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: cs/0410002</em>, Jul. 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"> S. Amari, <em class="ltx_emph ltx_font_italic">Information Geometry and Its Applications</em>, Tokyo, Japan: Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"> J. Martens and R. Grosse, ‚ÄúOptimizing neural networks with Kronecker-factored approximate curvature,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. 32nd ICML ‚Äô15</em>, Lille, France: ICML, Jul. 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"> R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. Manning, and C. Finn, ‚ÄúDirect preference optimization: Your language model is secretly a reward model,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2305.18290</em>, Jul. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"> D. Freedman, ‚ÄúOn tail probabilities for martingales,‚Äù <em class="ltx_emph ltx_font_italic">The Annals of Probability</em>, vol. 3, no. 1, pp. 100-118, Feb. 1975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"> D. Williams, <em class="ltx_emph ltx_font_italic">Probability with Martingales</em>. Cambridge, UK: Cambridge University Press, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"> D. Alvarez-Melis and T. Jaakkola, ‚ÄúGromov-Wasserstein alignment of word embedding spaces,‚Äù in <em class="ltx_emph ltx_font_italic">Proc. ACM EMNLP ‚Äô18</em>, Brussels, Belgium, Oct. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"> A. Lapidoth, <em class="ltx_emph ltx_font_italic">A Foundation in Digital Communication</em>. New York, NY, USA: Cambridge University Press, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"> T. Landauer, P. Foltz, and D. Laham, ‚ÄúAn introduction to latent semantic analysis,‚Äù <em class="ltx_emph ltx_font_italic">Discourse Processes</em>, vol. 25, no. 2-3, pp. 259-284, Jan. 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"> W. Johnson, J. Lindenstrauss, and G. Schechtman, ‚ÄúExtensions of Lipschitz maps into Banach spaces,‚Äù <em class="ltx_emph ltx_font_italic">Israel J. Math.</em>, vol. 54, no. 2, pp. 129-138, Jun. 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"> S. Foucart and H. Rauhut, <em class="ltx_emph ltx_font_italic">A Mathematical Introduction to Compressive Sensing</em>. New York, NY, USA: Birkh√§user, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"> F. Krahmer and R. Ward, ‚ÄúNew and improved Johnson-Lindenstrauss embeddings via the restricted isometry property,‚Äù <em class="ltx_emph ltx_font_italic">SIAM J. Math. Anal.</em>, vol. 43, no. 3, pp. 1269-1281, Jan. 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"> P. Elias, ‚ÄúPredictive coding - Part 1,‚Äù <em class="ltx_emph ltx_font_italic">IRE Trans. Inf. Theory</em>, vol. 1, no. 1, pp. 16-24, Mar. 1955.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"> P. Elias, ‚ÄúPredictive coding - Part 2,‚Äù <em class="ltx_emph ltx_font_italic">IRE Trans. Inf. Theory</em>, vol. 1, no. 1, pp. 24-33, Mar. 1955.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"> E. Jaynes, <em class="ltx_emph ltx_font_italic">Probability Theory: The Logic of Science</em>. New York, NY, USA: Cambridge University Press, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"> Y. Kim, C. Denton, L. Hoang, and A. Rush, ‚ÄúStructured attention networks,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 1702.00887</em>, Feb. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"> N. Macris and R. Urbanke, <em class="ltx_emph ltx_font_italic">Statistical Physics for Communications, Signal Processing, and Computer Science</em>. Lausanne, Swiss: √âcole Polytechnique F√©d√©rale de Lausanne, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"> J. Hopfield, ‚ÄúNeural networks and physical systems with emergent collective computational abilities,‚Äù <em class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, vol. 79, no. 8, pp. 2554-2558, Apr. 1982.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"> H. Ramsauer et al., ‚ÄúHopfield networks is all you need,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2008.02217</em>, Apr. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"> X. Niu, B. Bai, L. Deng, and W. Han, ‚ÄúBeyond scaling laws: Understanding transformer performance with associative memory,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2405.08707</em>, 14 May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock"> M. Donsker and S. Varadhan, ‚ÄúAsymptotic evaluation of certain markov process expectations for large time, IV,‚Äù <em class="ltx_emph ltx_font_italic">Comm. Pure Appl. Math.</em>, vol. 36, no. 2, pp. 183-212, Mar. 1983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock"> O. Luxembourg, D. Tsur, and H. Permuter, ‚ÄúTREET: Transfer entropy estimation via transformers,‚Äù <em class="ltx_emph ltx_font_italic">arXiv:2402.06919</em>, Jul. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock"> T. Lubik and C. Matthes, ‚ÄúTime-varying parameter vector autoregressions: Specification, estimation, and an application,‚Äù <em class="ltx_emph ltx_font_italic">Economic Quarterly</em>, vol. 101, no. 4, pp. 323-352, Q4 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock"> J. Haslbeck, L. Bringmann, and L. Waldorp, ‚ÄúA tutorial on estimating time-varying vector autoregressive models,‚Äù <em class="ltx_emph ltx_font_italic">Multivariate Behavioral Research</em>, vol. 56, no. 1, pp. 120-149, Jan. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock"> S. Yang, J. Kautz, and A. Hatamizadeh, ‚ÄúGated delta networks: Improving Mamba2 with delta rule,‚Äù <em class="ltx_emph ltx_font_italic">arXiv: 2412.06464</em>, Mar. 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"> J. Pearl and D. Mackenzie, <em class="ltx_emph ltx_font_italic">The Book of Why: The New Science of Cause and Effect</em>. New York, NY, USA: Basic Books, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock"> D. Kahneman, <em class="ltx_emph ltx_font_italic">Thinking, Fast and Slow</em>. New York, NY, USA: Farrar, Straus and Giroux, 2013.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Nov  3 03:55:57 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
